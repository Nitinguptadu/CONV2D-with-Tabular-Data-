{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, TimeDistributed, ConvLSTM2D, Reshape\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as sm\n",
    "import keras\n",
    "\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n",
       "0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n",
       "1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n",
       "2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n",
       "3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n",
       "4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n",
       "\n",
       "   X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     1     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 378 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  data.drop([\"ID\",\"X0\",\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X8\" ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y  X10  X11  X12  X13  X14  X15  X16  X17  X18  ...  X375  X376  X377  \\\n",
       "0  130.81    0    0    0    1    0    0    0    0    1  ...     0     0     1   \n",
       "1   88.53    0    0    0    0    0    0    0    0    1  ...     1     0     0   \n",
       "2   76.26    0    0    0    0    0    0    0    1    0  ...     0     0     0   \n",
       "3   80.62    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "4   78.02    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "\n",
       "   X378  X379  X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0  \n",
       "2     0     0     0     1     0     0     0  \n",
       "3     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 369)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "      <td>ai</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "      <td>g</td>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>j</td>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>az</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>z</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>w</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>i</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>8410</td>\n",
       "      <td>aj</td>\n",
       "      <td>h</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>j</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>8411</td>\n",
       "      <td>t</td>\n",
       "      <td>aa</td>\n",
       "      <td>ai</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>j</td>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>8413</td>\n",
       "      <td>y</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>8414</td>\n",
       "      <td>ak</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>c</td>\n",
       "      <td>q</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>8416</td>\n",
       "      <td>t</td>\n",
       "      <td>aa</td>\n",
       "      <td>ai</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>r</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows Ã— 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  X0  X1  X2 X3 X4  X5 X6 X8  X10  ...  X375  X376  X377  X378  \\\n",
       "0        1  az   v   n  f  d   t  a  w    0  ...     0     0     0     1   \n",
       "1        2   t   b  ai  a  d   b  g  y    0  ...     0     0     1     0   \n",
       "2        3  az   v  as  f  d   a  j  j    0  ...     0     0     0     1   \n",
       "3        4  az   l   n  f  d   z  l  n    0  ...     0     0     0     1   \n",
       "4        5   w   s  as  c  d   y  i  m    0  ...     1     0     0     0   \n",
       "...    ...  ..  ..  .. .. ..  .. .. ..  ...  ...   ...   ...   ...   ...   \n",
       "4204  8410  aj   h  as  f  d  aa  j  e    0  ...     0     0     0     0   \n",
       "4205  8411   t  aa  ai  d  d  aa  j  y    0  ...     0     1     0     0   \n",
       "4206  8413   y   v  as  f  d  aa  d  w    0  ...     0     0     0     0   \n",
       "4207  8414  ak   v  as  a  d  aa  c  q    0  ...     0     0     1     0   \n",
       "4208  8416   t  aa  ai  c  d  aa  g  r    0  ...     1     0     0     0   \n",
       "\n",
       "      X379  X380  X382  X383  X384  X385  \n",
       "0        0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...  \n",
       "4204     0     0     0     0     0     0  \n",
       "4205     0     0     0     0     0     0  \n",
       "4206     0     0     0     0     0     0  \n",
       "4207     0     0     0     0     0     0  \n",
       "4208     0     0     0     0     0     0  \n",
       "\n",
       "[4209 rows x 377 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest =  dtest.drop([\"ID\",\"X0\",\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X8\" ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X10  X11  X12  X13  X14  X15  X16  X17  X18  X19  ...  X375  X376  X377  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "1    0    0    0    0    0    0    0    0    0    1  ...     0     0     1   \n",
       "2    0    0    0    0    1    0    0    0    0    0  ...     0     0     0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "4    0    0    0    0    1    0    0    0    0    0  ...     1     0     0   \n",
       "\n",
       "   X378  X379  X380  X382  X383  X384  X385  \n",
       "0     1     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0  \n",
       "2     1     0     0     0     0     0     0  \n",
       "3     1     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 368 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 368)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data.values[0:3499,1:], data.values[0:3499, :1].ravel()\n",
    "X_valid, y_valid = data.values[3500:4208,1:], data.values[3500:4208, :1].ravel()\n",
    "X_test = dtest.values[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train---- (3499, 368)\n",
      "y_train---- (3499,)\n",
      "X_valid---- (708, 368)\n",
      "y_valid---- (708,)\n",
      "X_test----- (4209, 368)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train----\",X_train.shape)\n",
    "print(\"y_train----\",y_train.shape)      \n",
    "print(\"X_valid----\",X_valid.shape)\n",
    "print(\"y_valid----\",y_valid.shape)\n",
    "print(\"X_test-----\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.81"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = min_max_scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred, y_test):\n",
    "    evs = sm.explained_variance_score(y_test, pred)\n",
    "    me = sm.max_error(y_test, pred)\n",
    "    mae = sm.mean_absolute_error(y_test, pred)\n",
    "    mse = sm.mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #msle = sm.mean_squared_log_error(y_test, pred)\n",
    "    m_ae = sm.median_absolute_error(y_test, pred)\n",
    "    r2 = sm.r2_score(y_test, pred)\n",
    "    #mpd = sm.mean_poisson_deviance(y_test, pred)\n",
    "    #mgd = sm.mean_gamma_deviance(y_test, pred)\n",
    "    mape = mean_absolute_percentage_error(pred, y_test)\n",
    "    return({'Explained Variance Score': evs,\n",
    "            'Max Error': me,\n",
    "            'Mean Absolute Error': mae,\n",
    "            'Mean Squared Error': mse,\n",
    "            'Root Mean Squared Error': rmse,\n",
    "            #'Mean Squared Log Error': msle,\n",
    "            'Median Absolute Error': m_ae,\n",
    "            'RÂ² Score': r2,\n",
    "            #'Mean Poisson Deviance': mpd,\n",
    "            #'Mean Gamma Deviance': mgd,\n",
    "            'Mean Absolute Percentage Error': mape\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequences = 2\n",
    "timesteps = X_train.shape[1]//subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], subsequences, 1, timesteps, 1))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], subsequences, 1, timesteps, 1))\n",
    "X_test  = X_test.reshape((X_test.shape[0], subsequences, 1, timesteps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3499, 2, 1, 184, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 708 samples\n",
      "Epoch 1/700\n",
      " - 34s - loss: 10287.8766 - val_loss: 9645.3753\n",
      "Epoch 2/700\n",
      " - 21s - loss: 9529.9907 - val_loss: 7993.5027\n",
      "Epoch 3/700\n",
      " - 21s - loss: 6649.7814 - val_loss: 3300.6846\n",
      "Epoch 4/700\n",
      " - 21s - loss: 1436.7710 - val_loss: 1031.3773\n",
      "Epoch 5/700\n",
      " - 21s - loss: 1089.6176 - val_loss: 186.1676\n",
      "Epoch 6/700\n",
      " - 21s - loss: 326.9648 - val_loss: 455.2619\n",
      "Epoch 7/700\n",
      " - 21s - loss: 406.2818 - val_loss: 134.1756\n",
      "Epoch 8/700\n",
      " - 21s - loss: 213.8799 - val_loss: 265.9983\n",
      "Epoch 9/700\n",
      " - 21s - loss: 205.1158 - val_loss: 132.6763\n",
      "Epoch 10/700\n",
      " - 21s - loss: 192.5088 - val_loss: 136.2464\n",
      "Epoch 11/700\n",
      " - 21s - loss: 172.8127 - val_loss: 152.5013\n",
      "Epoch 12/700\n",
      " - 21s - loss: 173.2925 - val_loss: 133.6840\n",
      "Epoch 13/700\n",
      " - 21s - loss: 168.2749 - val_loss: 130.2755\n",
      "Epoch 14/700\n",
      " - 21s - loss: 166.8993 - val_loss: 136.2322\n",
      "Epoch 15/700\n",
      " - 21s - loss: 166.6879 - val_loss: 133.2881\n",
      "Epoch 16/700\n",
      " - 21s - loss: 165.9325 - val_loss: 130.4821\n",
      "Epoch 17/700\n",
      " - 21s - loss: 165.4653 - val_loss: 133.1760\n",
      "Epoch 18/700\n",
      " - 21s - loss: 165.3812 - val_loss: 132.6065\n",
      "Epoch 19/700\n",
      " - 21s - loss: 164.9764 - val_loss: 130.3913\n",
      "Epoch 20/700\n",
      " - 21s - loss: 164.7113 - val_loss: 132.0802\n",
      "Epoch 21/700\n",
      " - 21s - loss: 164.5046 - val_loss: 130.8525\n",
      "Epoch 22/700\n",
      " - 21s - loss: 164.0944 - val_loss: 131.4234\n",
      "Epoch 23/700\n",
      " - 21s - loss: 163.8240 - val_loss: 129.9827\n",
      "Epoch 24/700\n",
      " - 21s - loss: 163.5421 - val_loss: 129.9218\n",
      "Epoch 25/700\n",
      " - 21s - loss: 163.2666 - val_loss: 130.2776\n",
      "Epoch 26/700\n",
      " - 21s - loss: 163.0683 - val_loss: 129.3117\n",
      "Epoch 27/700\n",
      " - 21s - loss: 162.9896 - val_loss: 129.9722\n",
      "Epoch 28/700\n",
      " - 21s - loss: 162.3057 - val_loss: 127.5713\n",
      "Epoch 29/700\n",
      " - 21s - loss: 162.0363 - val_loss: 130.1191\n",
      "Epoch 30/700\n",
      " - 21s - loss: 161.9166 - val_loss: 127.9598\n",
      "Epoch 31/700\n",
      " - 21s - loss: 161.5040 - val_loss: 128.7599\n",
      "Epoch 32/700\n",
      " - 21s - loss: 161.0702 - val_loss: 126.9085\n",
      "Epoch 33/700\n",
      " - 21s - loss: 160.7376 - val_loss: 128.2908\n",
      "Epoch 34/700\n",
      " - 21s - loss: 160.4561 - val_loss: 126.6424\n",
      "Epoch 35/700\n",
      " - 21s - loss: 160.3183 - val_loss: 127.2454\n",
      "Epoch 36/700\n",
      " - 21s - loss: 159.8532 - val_loss: 126.3623\n",
      "Epoch 37/700\n",
      " - 21s - loss: 159.7132 - val_loss: 126.1839\n",
      "Epoch 38/700\n",
      " - 21s - loss: 159.3483 - val_loss: 126.8803\n",
      "Epoch 39/700\n",
      " - 21s - loss: 158.9129 - val_loss: 123.9447\n",
      "Epoch 40/700\n",
      " - 21s - loss: 159.0154 - val_loss: 126.7729\n",
      "Epoch 41/700\n",
      " - 21s - loss: 157.9638 - val_loss: 123.4054\n",
      "Epoch 42/700\n",
      " - 21s - loss: 157.9787 - val_loss: 125.3181\n",
      "Epoch 43/700\n",
      " - 21s - loss: 157.4075 - val_loss: 123.5753\n",
      "Epoch 44/700\n",
      " - 21s - loss: 156.9839 - val_loss: 124.5314\n",
      "Epoch 45/700\n",
      " - 21s - loss: 156.6751 - val_loss: 122.5873\n",
      "Epoch 46/700\n",
      " - 27s - loss: 156.4669 - val_loss: 124.6289\n",
      "Epoch 47/700\n",
      " - 22s - loss: 156.0949 - val_loss: 122.1883\n",
      "Epoch 48/700\n",
      " - 21s - loss: 155.5885 - val_loss: 121.8743\n",
      "Epoch 49/700\n",
      " - 21s - loss: 155.6544 - val_loss: 122.3509\n",
      "Epoch 50/700\n",
      " - 21s - loss: 155.1659 - val_loss: 120.7976\n",
      "Epoch 51/700\n",
      " - 22s - loss: 154.4451 - val_loss: 122.2503\n",
      "Epoch 52/700\n",
      " - 21s - loss: 154.0956 - val_loss: 119.4091\n",
      "Epoch 53/700\n",
      " - 21s - loss: 153.3051 - val_loss: 122.4927\n",
      "Epoch 54/700\n",
      " - 21s - loss: 153.1903 - val_loss: 119.4933\n",
      "Epoch 55/700\n",
      " - 21s - loss: 152.7411 - val_loss: 119.3341\n",
      "Epoch 56/700\n",
      " - 21s - loss: 152.4046 - val_loss: 118.9810\n",
      "Epoch 57/700\n",
      " - 21s - loss: 151.9321 - val_loss: 118.2373\n",
      "Epoch 58/700\n",
      " - 21s - loss: 152.0349 - val_loss: 117.0552\n",
      "Epoch 59/700\n",
      " - 21s - loss: 151.6123 - val_loss: 118.7325\n",
      "Epoch 60/700\n",
      " - 21s - loss: 151.0558 - val_loss: 116.3864\n",
      "Epoch 61/700\n",
      " - 21s - loss: 150.4831 - val_loss: 117.2496\n",
      "Epoch 62/700\n",
      " - 21s - loss: 149.9287 - val_loss: 115.4532\n",
      "Epoch 63/700\n",
      " - 21s - loss: 149.1143 - val_loss: 115.4848\n",
      "Epoch 64/700\n",
      " - 21s - loss: 148.4970 - val_loss: 115.1236\n",
      "Epoch 65/700\n",
      " - 21s - loss: 147.8001 - val_loss: 114.3153\n",
      "Epoch 66/700\n",
      " - 22s - loss: 147.4345 - val_loss: 114.9962\n",
      "Epoch 67/700\n",
      " - 21s - loss: 146.8298 - val_loss: 112.8403\n",
      "Epoch 68/700\n",
      " - 21s - loss: 146.1820 - val_loss: 112.8572\n",
      "Epoch 69/700\n",
      " - 21s - loss: 145.5640 - val_loss: 111.9888\n",
      "Epoch 70/700\n",
      " - 21s - loss: 144.9773 - val_loss: 110.9643\n",
      "Epoch 71/700\n",
      " - 21s - loss: 144.3511 - val_loss: 111.0991\n",
      "Epoch 72/700\n",
      " - 21s - loss: 143.7572 - val_loss: 109.5645\n",
      "Epoch 73/700\n",
      " - 21s - loss: 143.0217 - val_loss: 109.4697\n",
      "Epoch 74/700\n",
      " - 21s - loss: 142.3951 - val_loss: 109.9669\n",
      "Epoch 75/700\n",
      " - 21s - loss: 141.7118 - val_loss: 108.2561\n",
      "Epoch 76/700\n",
      " - 21s - loss: 141.3991 - val_loss: 110.0662\n",
      "Epoch 77/700\n",
      " - 21s - loss: 140.5907 - val_loss: 104.7229\n",
      "Epoch 78/700\n",
      " - 21s - loss: 140.0539 - val_loss: 111.4788\n",
      "Epoch 79/700\n",
      " - 21s - loss: 140.3112 - val_loss: 102.3834\n",
      "Epoch 80/700\n",
      " - 21s - loss: 138.6169 - val_loss: 105.9625\n",
      "Epoch 81/700\n",
      " - 21s - loss: 137.6340 - val_loss: 103.8825\n",
      "Epoch 82/700\n",
      " - 21s - loss: 136.5822 - val_loss: 101.5145\n",
      "Epoch 83/700\n",
      " - 21s - loss: 135.6981 - val_loss: 102.5665\n",
      "Epoch 84/700\n",
      " - 21s - loss: 134.9309 - val_loss: 101.2681\n",
      "Epoch 85/700\n",
      " - 21s - loss: 134.1634 - val_loss: 99.0455\n",
      "Epoch 86/700\n",
      " - 21s - loss: 133.0578 - val_loss: 100.3438\n",
      "Epoch 87/700\n",
      " - 21s - loss: 132.1940 - val_loss: 97.0123\n",
      "Epoch 88/700\n",
      " - 21s - loss: 131.6574 - val_loss: 95.6397\n",
      "Epoch 89/700\n",
      " - 21s - loss: 130.5340 - val_loss: 98.5087\n",
      "Epoch 90/700\n",
      " - 21s - loss: 129.4641 - val_loss: 93.0450\n",
      "Epoch 91/700\n",
      " - 21s - loss: 128.6831 - val_loss: 97.5557\n",
      "Epoch 92/700\n",
      " - 21s - loss: 127.8790 - val_loss: 92.3659\n",
      "Epoch 93/700\n",
      " - 21s - loss: 126.8955 - val_loss: 91.2129\n",
      "Epoch 94/700\n",
      " - 21s - loss: 125.9777 - val_loss: 92.5597\n",
      "Epoch 95/700\n",
      " - 21s - loss: 124.6251 - val_loss: 90.1249\n",
      "Epoch 96/700\n",
      " - 21s - loss: 123.5473 - val_loss: 89.6637\n",
      "Epoch 97/700\n",
      " - 21s - loss: 122.6447 - val_loss: 87.9664\n",
      "Epoch 98/700\n",
      " - 21s - loss: 121.7186 - val_loss: 89.4091\n",
      "Epoch 99/700\n",
      " - 21s - loss: 121.0001 - val_loss: 84.8537\n",
      "Epoch 100/700\n",
      " - 21s - loss: 119.6440 - val_loss: 85.7993\n",
      "Epoch 101/700\n",
      " - 21s - loss: 118.6068 - val_loss: 83.6975\n",
      "Epoch 102/700\n",
      " - 21s - loss: 117.5512 - val_loss: 84.0022\n",
      "Epoch 103/700\n",
      " - 21s - loss: 116.6709 - val_loss: 81.9228\n",
      "Epoch 104/700\n",
      " - 21s - loss: 115.6026 - val_loss: 81.0250\n",
      "Epoch 105/700\n",
      " - 21s - loss: 114.8226 - val_loss: 77.6089\n",
      "Epoch 106/700\n",
      " - 21s - loss: 114.2596 - val_loss: 78.0258\n",
      "Epoch 107/700\n",
      " - 21s - loss: 113.1350 - val_loss: 81.1066\n",
      "Epoch 108/700\n",
      " - 21s - loss: 112.3056 - val_loss: 78.6101\n",
      "Epoch 109/700\n",
      " - 21s - loss: 111.0438 - val_loss: 74.0568\n",
      "Epoch 110/700\n",
      " - 21s - loss: 110.5335 - val_loss: 78.6819\n",
      "Epoch 111/700\n",
      " - 21s - loss: 109.2444 - val_loss: 73.3365\n",
      "Epoch 112/700\n",
      " - 21s - loss: 108.2291 - val_loss: 73.4445\n",
      "Epoch 113/700\n",
      " - 21s - loss: 107.5555 - val_loss: 75.2068\n",
      "Epoch 114/700\n",
      " - 21s - loss: 106.4952 - val_loss: 69.9002\n",
      "Epoch 115/700\n",
      " - 21s - loss: 105.7206 - val_loss: 74.0367\n",
      "Epoch 116/700\n",
      " - 21s - loss: 104.8663 - val_loss: 69.1923\n",
      "Epoch 117/700\n",
      " - 21s - loss: 104.2321 - val_loss: 71.2044\n",
      "Epoch 118/700\n",
      " - 21s - loss: 103.5213 - val_loss: 70.0498\n",
      "Epoch 119/700\n",
      " - 21s - loss: 102.6565 - val_loss: 68.8899\n",
      "Epoch 120/700\n",
      " - 21s - loss: 102.2576 - val_loss: 64.1718\n",
      "Epoch 121/700\n",
      " - 21s - loss: 102.0336 - val_loss: 65.7482\n",
      "Epoch 122/700\n",
      " - 21s - loss: 100.9957 - val_loss: 70.2144\n",
      "Epoch 123/700\n",
      " - 21s - loss: 100.2238 - val_loss: 70.9653\n",
      "Epoch 124/700\n",
      " - 21s - loss: 100.6840 - val_loss: 63.0688\n",
      "Epoch 125/700\n",
      " - 21s - loss: 98.9958 - val_loss: 62.3195\n",
      "Epoch 126/700\n",
      " - 21s - loss: 97.6910 - val_loss: 68.2007\n",
      "Epoch 127/700\n",
      " - 21s - loss: 97.5786 - val_loss: 68.6889\n",
      "Epoch 128/700\n",
      " - 21s - loss: 96.7959 - val_loss: 64.6859\n",
      "Epoch 129/700\n",
      " - 21s - loss: 96.1907 - val_loss: 60.2960\n",
      "Epoch 130/700\n",
      " - 21s - loss: 95.3187 - val_loss: 60.5751\n",
      "Epoch 131/700\n",
      " - 21s - loss: 95.1461 - val_loss: 64.0937\n",
      "Epoch 132/700\n",
      " - 21s - loss: 93.9721 - val_loss: 60.7039\n",
      "Epoch 133/700\n",
      " - 21s - loss: 93.5615 - val_loss: 61.3790\n",
      "Epoch 134/700\n",
      " - 21s - loss: 93.0373 - val_loss: 62.0312\n",
      "Epoch 135/700\n",
      " - 21s - loss: 92.6308 - val_loss: 71.9362\n",
      "Epoch 136/700\n",
      " - 21s - loss: 92.7234 - val_loss: 58.2115\n",
      "Epoch 137/700\n",
      " - 21s - loss: 91.3267 - val_loss: 57.7637\n",
      "Epoch 138/700\n",
      " - 21s - loss: 90.7013 - val_loss: 55.3941\n",
      "Epoch 139/700\n",
      " - 21s - loss: 90.4905 - val_loss: 60.6049\n",
      "Epoch 140/700\n",
      " - 21s - loss: 90.1172 - val_loss: 61.1996\n",
      "Epoch 141/700\n",
      " - 21s - loss: 89.1466 - val_loss: 64.0307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/700\n",
      " - 21s - loss: 89.0139 - val_loss: 62.7601\n",
      "Epoch 143/700\n",
      " - 21s - loss: 88.6318 - val_loss: 55.1429\n",
      "Epoch 144/700\n",
      " - 21s - loss: 88.4791 - val_loss: 53.2129\n",
      "Epoch 145/700\n",
      " - 21s - loss: 86.8328 - val_loss: 55.9953\n",
      "Epoch 146/700\n",
      " - 21s - loss: 86.3850 - val_loss: 56.6816\n",
      "Epoch 147/700\n",
      " - 21s - loss: 86.1541 - val_loss: 56.2934\n",
      "Epoch 148/700\n",
      " - 21s - loss: 85.4269 - val_loss: 61.8919\n",
      "Epoch 149/700\n",
      " - 21s - loss: 85.4902 - val_loss: 55.0792\n",
      "Epoch 150/700\n",
      " - 21s - loss: 84.7595 - val_loss: 51.8488\n",
      "Epoch 151/700\n",
      " - 21s - loss: 84.9142 - val_loss: 55.0425\n",
      "Epoch 152/700\n",
      " - 21s - loss: 84.4977 - val_loss: 54.8991\n",
      "Epoch 153/700\n",
      " - 21s - loss: 84.1954 - val_loss: 58.1087\n",
      "Epoch 154/700\n",
      " - 21s - loss: 84.2042 - val_loss: 58.0961\n",
      "Epoch 155/700\n",
      " - 21s - loss: 84.1108 - val_loss: 56.2299\n",
      "Epoch 156/700\n",
      " - 21s - loss: 83.2827 - val_loss: 58.3119\n",
      "Epoch 157/700\n",
      " - 21s - loss: 83.1873 - val_loss: 56.9416\n",
      "Epoch 158/700\n",
      " - 21s - loss: 83.0122 - val_loss: 60.8720\n",
      "Epoch 159/700\n",
      " - 21s - loss: 83.8147 - val_loss: 66.4455\n",
      "Epoch 160/700\n",
      " - 21s - loss: 83.8437 - val_loss: 59.4031\n",
      "Epoch 161/700\n",
      " - 21s - loss: 82.5988 - val_loss: 60.8355\n",
      "Epoch 162/700\n",
      " - 21s - loss: 82.7644 - val_loss: 61.1806\n",
      "Epoch 163/700\n",
      " - 21s - loss: 82.1994 - val_loss: 55.1180\n",
      "Epoch 164/700\n",
      " - 21s - loss: 82.0612 - val_loss: 50.8267\n",
      "Epoch 165/700\n",
      " - 21s - loss: 83.2316 - val_loss: 49.7971\n",
      "Epoch 166/700\n",
      " - 21s - loss: 82.1808 - val_loss: 54.1180\n",
      "Epoch 167/700\n",
      " - 21s - loss: 82.4636 - val_loss: 57.9590\n",
      "Epoch 168/700\n",
      " - 21s - loss: 81.7522 - val_loss: 54.7304\n",
      "Epoch 169/700\n",
      " - 21s - loss: 81.3247 - val_loss: 51.5879\n",
      "Epoch 170/700\n",
      " - 21s - loss: 82.0000 - val_loss: 49.4171\n",
      "Epoch 171/700\n",
      " - 21s - loss: 82.0864 - val_loss: 52.8348\n",
      "Epoch 172/700\n",
      " - 21s - loss: 81.7858 - val_loss: 53.8649\n",
      "Epoch 173/700\n",
      " - 21s - loss: 81.1288 - val_loss: 58.3935\n",
      "Epoch 174/700\n",
      " - 21s - loss: 81.1006 - val_loss: 53.8614\n",
      "Epoch 175/700\n",
      " - 21s - loss: 80.8757 - val_loss: 57.7961\n",
      "Epoch 176/700\n",
      " - 21s - loss: 81.1086 - val_loss: 58.2927\n",
      "Epoch 177/700\n",
      " - 21s - loss: 80.7683 - val_loss: 52.2322\n",
      "Epoch 178/700\n",
      " - 21s - loss: 81.3785 - val_loss: 50.9058\n",
      "Epoch 179/700\n",
      " - 21s - loss: 81.5327 - val_loss: 48.8730\n",
      "Epoch 180/700\n",
      " - 21s - loss: 81.6500 - val_loss: 48.5792\n",
      "Epoch 181/700\n",
      " - 21s - loss: 81.6290 - val_loss: 49.7155\n",
      "Epoch 182/700\n",
      " - 21s - loss: 80.9273 - val_loss: 52.9452\n",
      "Epoch 183/700\n",
      " - 21s - loss: 80.8311 - val_loss: 55.6124\n",
      "Epoch 184/700\n",
      " - 21s - loss: 80.1616 - val_loss: 51.6013\n",
      "Epoch 185/700\n",
      " - 21s - loss: 80.0943 - val_loss: 53.5321\n",
      "Epoch 186/700\n",
      " - 21s - loss: 80.3878 - val_loss: 54.8338\n",
      "Epoch 187/700\n",
      " - 21s - loss: 80.2036 - val_loss: 57.9429\n",
      "Epoch 188/700\n",
      " - 21s - loss: 80.4733 - val_loss: 60.5992\n",
      "Epoch 189/700\n",
      " - 21s - loss: 80.0411 - val_loss: 51.7200\n",
      "Epoch 190/700\n",
      " - 21s - loss: 79.6356 - val_loss: 57.2326\n",
      "Epoch 191/700\n",
      " - 21s - loss: 79.7440 - val_loss: 53.9258\n",
      "Epoch 192/700\n",
      " - 21s - loss: 79.8934 - val_loss: 59.0223\n",
      "Epoch 193/700\n",
      " - 21s - loss: 80.0501 - val_loss: 59.4669\n",
      "Epoch 194/700\n",
      " - 21s - loss: 79.7113 - val_loss: 51.1490\n",
      "Epoch 195/700\n",
      " - 21s - loss: 80.1401 - val_loss: 51.6655\n",
      "Epoch 196/700\n",
      " - 21s - loss: 79.7627 - val_loss: 50.3624\n",
      "Epoch 197/700\n",
      " - 21s - loss: 80.8733 - val_loss: 47.6370\n",
      "Epoch 198/700\n",
      " - 21s - loss: 80.4578 - val_loss: 48.9756\n",
      "Epoch 199/700\n",
      " - 21s - loss: 81.2167 - val_loss: 48.4309\n",
      "Epoch 200/700\n",
      " - 21s - loss: 80.4004 - val_loss: 49.9792\n",
      "Epoch 201/700\n",
      " - 21s - loss: 79.0941 - val_loss: 57.7716\n",
      "Epoch 202/700\n",
      " - 21s - loss: 80.6035 - val_loss: 58.1343\n",
      "Epoch 203/700\n",
      " - 21s - loss: 78.9083 - val_loss: 58.5048\n",
      "Epoch 204/700\n",
      " - 21s - loss: 79.1032 - val_loss: 54.9880\n",
      "Epoch 205/700\n",
      " - 21s - loss: 79.0009 - val_loss: 50.0481\n",
      "Epoch 206/700\n",
      " - 21s - loss: 79.3524 - val_loss: 49.1668\n",
      "Epoch 207/700\n",
      " - 21s - loss: 79.3313 - val_loss: 49.2027\n",
      "Epoch 208/700\n",
      " - 21s - loss: 79.0005 - val_loss: 50.4672\n",
      "Epoch 209/700\n",
      " - 21s - loss: 78.7593 - val_loss: 49.9769\n",
      "Epoch 210/700\n",
      " - 21s - loss: 79.1039 - val_loss: 49.7262\n",
      "Epoch 211/700\n",
      " - 21s - loss: 78.6822 - val_loss: 51.1519\n",
      "Epoch 212/700\n",
      " - 21s - loss: 78.3772 - val_loss: 56.1178\n",
      "Epoch 213/700\n",
      " - 21s - loss: 78.4916 - val_loss: 50.2395\n",
      "Epoch 214/700\n",
      " - 21s - loss: 78.8266 - val_loss: 47.7339\n",
      "Epoch 215/700\n",
      " - 21s - loss: 80.1338 - val_loss: 48.5578\n",
      "Epoch 216/700\n",
      " - 21s - loss: 79.2061 - val_loss: 48.0723\n",
      "Epoch 217/700\n",
      " - 21s - loss: 79.5588 - val_loss: 47.6370\n",
      "Epoch 218/700\n",
      " - 21s - loss: 79.3330 - val_loss: 48.7859\n",
      "Epoch 219/700\n",
      " - 21s - loss: 78.7623 - val_loss: 58.2273\n",
      "Epoch 220/700\n",
      " - 21s - loss: 78.9336 - val_loss: 57.2712\n",
      "Epoch 221/700\n",
      " - 21s - loss: 78.7719 - val_loss: 58.1572\n",
      "Epoch 222/700\n",
      " - 21s - loss: 78.8087 - val_loss: 56.6779\n",
      "Epoch 223/700\n",
      " - 21s - loss: 78.1408 - val_loss: 51.5061\n",
      "Epoch 224/700\n",
      " - 21s - loss: 78.2858 - val_loss: 50.9390\n",
      "Epoch 225/700\n",
      " - 21s - loss: 77.9143 - val_loss: 50.6020\n",
      "Epoch 226/700\n",
      " - 21s - loss: 78.3749 - val_loss: 54.4324\n",
      "Epoch 227/700\n",
      " - 21s - loss: 78.6841 - val_loss: 52.3925\n",
      "Epoch 228/700\n",
      " - 21s - loss: 78.9829 - val_loss: 51.1296\n",
      "Epoch 229/700\n",
      " - 21s - loss: 77.9696 - val_loss: 48.9919\n",
      "Epoch 230/700\n",
      " - 21s - loss: 79.3152 - val_loss: 52.9096\n",
      "Epoch 231/700\n",
      " - 21s - loss: 78.3989 - val_loss: 54.6159\n",
      "Epoch 232/700\n",
      " - 21s - loss: 78.1471 - val_loss: 62.8121\n",
      "Epoch 233/700\n",
      " - 21s - loss: 80.6786 - val_loss: 57.5846\n",
      "Epoch 234/700\n",
      " - 21s - loss: 79.6364 - val_loss: 51.8699\n",
      "Epoch 235/700\n",
      " - 21s - loss: 77.6541 - val_loss: 52.1407\n",
      "Epoch 236/700\n",
      " - 21s - loss: 77.4614 - val_loss: 52.5626\n",
      "Epoch 237/700\n",
      " - 21s - loss: 77.5243 - val_loss: 52.0615\n",
      "Epoch 238/700\n",
      " - 21s - loss: 77.3648 - val_loss: 51.3038\n",
      "Epoch 239/700\n",
      " - 21s - loss: 77.4992 - val_loss: 47.3687\n",
      "Epoch 240/700\n",
      " - 21s - loss: 78.5580 - val_loss: 47.5349\n",
      "Epoch 241/700\n",
      " - 21s - loss: 77.8802 - val_loss: 49.2575\n",
      "Epoch 242/700\n",
      " - 21s - loss: 77.4614 - val_loss: 52.4495\n",
      "Epoch 243/700\n",
      " - 21s - loss: 77.9787 - val_loss: 53.4571\n",
      "Epoch 244/700\n",
      " - 21s - loss: 78.4837 - val_loss: 63.7286\n",
      "Epoch 245/700\n",
      " - 21s - loss: 79.1737 - val_loss: 61.7012\n",
      "Epoch 246/700\n",
      " - 21s - loss: 77.4136 - val_loss: 61.1184\n",
      "Epoch 247/700\n",
      " - 21s - loss: 78.8738 - val_loss: 67.1070\n",
      "Epoch 248/700\n",
      " - 21s - loss: 80.1705 - val_loss: 58.3542\n",
      "Epoch 249/700\n",
      " - 21s - loss: 79.4815 - val_loss: 53.8589\n",
      "Epoch 250/700\n",
      " - 21s - loss: 78.4593 - val_loss: 48.4833\n",
      "Epoch 251/700\n",
      " - 21s - loss: 77.8637 - val_loss: 50.8281\n",
      "Epoch 252/700\n",
      " - 21s - loss: 76.9237 - val_loss: 52.4590\n",
      "Epoch 253/700\n",
      " - 21s - loss: 76.8795 - val_loss: 48.3012\n",
      "Epoch 254/700\n",
      " - 21s - loss: 77.4750 - val_loss: 47.2068\n",
      "Epoch 255/700\n",
      " - 21s - loss: 77.9134 - val_loss: 48.5226\n",
      "Epoch 256/700\n",
      " - 21s - loss: 76.8296 - val_loss: 49.2238\n",
      "Epoch 257/700\n",
      " - 21s - loss: 77.8648 - val_loss: 49.3631\n",
      "Epoch 258/700\n",
      " - 21s - loss: 78.2663 - val_loss: 54.3509\n",
      "Epoch 259/700\n",
      " - 21s - loss: 77.4319 - val_loss: 56.6603\n",
      "Epoch 260/700\n",
      " - 21s - loss: 77.0504 - val_loss: 47.8526\n",
      "Epoch 261/700\n",
      " - 21s - loss: 77.2132 - val_loss: 46.4833\n",
      "Epoch 262/700\n",
      " - 21s - loss: 79.0617 - val_loss: 48.0030\n",
      "Epoch 263/700\n",
      " - 21s - loss: 77.1398 - val_loss: 51.6326\n",
      "Epoch 264/700\n",
      " - 21s - loss: 76.6861 - val_loss: 50.9264\n",
      "Epoch 265/700\n",
      " - 21s - loss: 76.7751 - val_loss: 47.9782\n",
      "Epoch 266/700\n",
      " - 21s - loss: 77.1031 - val_loss: 47.6602\n",
      "Epoch 267/700\n",
      " - 21s - loss: 76.8360 - val_loss: 49.1967\n",
      "Epoch 268/700\n",
      " - 21s - loss: 76.5588 - val_loss: 50.9765\n",
      "Epoch 269/700\n",
      " - 21s - loss: 76.5580 - val_loss: 49.3182\n",
      "Epoch 270/700\n",
      " - 21s - loss: 77.1699 - val_loss: 48.4960\n",
      "Epoch 271/700\n",
      " - 21s - loss: 77.5186 - val_loss: 54.0018\n",
      "Epoch 272/700\n",
      " - 21s - loss: 78.0808 - val_loss: 73.2185\n",
      "Epoch 273/700\n",
      " - 21s - loss: 79.9037 - val_loss: 65.6218\n",
      "Epoch 274/700\n",
      " - 21s - loss: 79.4524 - val_loss: 57.6058\n",
      "Epoch 275/700\n",
      " - 21s - loss: 79.3324 - val_loss: 48.1599\n",
      "Epoch 276/700\n",
      " - 21s - loss: 78.2923 - val_loss: 46.5718\n",
      "Epoch 277/700\n",
      " - 21s - loss: 77.5685 - val_loss: 46.3514\n",
      "Epoch 278/700\n",
      " - 21s - loss: 79.2101 - val_loss: 47.7269\n",
      "Epoch 279/700\n",
      " - 21s - loss: 79.6121 - val_loss: 58.4802\n",
      "Epoch 280/700\n",
      " - 21s - loss: 78.8180 - val_loss: 62.1041\n",
      "Epoch 281/700\n",
      " - 21s - loss: 76.6625 - val_loss: 50.8021\n",
      "Epoch 282/700\n",
      " - 21s - loss: 77.2509 - val_loss: 56.2682\n",
      "Epoch 283/700\n",
      " - 21s - loss: 79.2683 - val_loss: 48.8357\n",
      "Epoch 284/700\n",
      " - 21s - loss: 76.3565 - val_loss: 52.5274\n",
      "Epoch 285/700\n",
      " - 21s - loss: 76.2260 - val_loss: 47.2873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/700\n",
      " - 21s - loss: 76.6489 - val_loss: 48.3571\n",
      "Epoch 287/700\n",
      " - 21s - loss: 76.8731 - val_loss: 47.8294\n",
      "Epoch 288/700\n",
      " - 21s - loss: 76.9683 - val_loss: 47.3731\n",
      "Epoch 289/700\n",
      " - 21s - loss: 77.4788 - val_loss: 48.5509\n",
      "Epoch 290/700\n",
      " - 21s - loss: 78.0975 - val_loss: 57.7284\n",
      "Epoch 291/700\n",
      " - 21s - loss: 77.4487 - val_loss: 55.6522\n",
      "Epoch 292/700\n",
      " - 21s - loss: 76.8700 - val_loss: 54.0966\n",
      "Epoch 293/700\n",
      " - 21s - loss: 76.3649 - val_loss: 53.5355\n",
      "Epoch 294/700\n",
      " - 21s - loss: 76.2876 - val_loss: 51.1777\n",
      "Epoch 295/700\n",
      " - 21s - loss: 76.1397 - val_loss: 52.1214\n",
      "Epoch 296/700\n",
      " - 21s - loss: 76.4149 - val_loss: 48.3436\n",
      "Epoch 297/700\n",
      " - 21s - loss: 76.3876 - val_loss: 51.6416\n",
      "Epoch 298/700\n",
      " - 21s - loss: 76.5114 - val_loss: 54.0414\n",
      "Epoch 299/700\n",
      " - 21s - loss: 76.1113 - val_loss: 53.4481\n",
      "Epoch 300/700\n",
      " - 21s - loss: 76.5746 - val_loss: 51.1600\n",
      "Epoch 301/700\n",
      " - 21s - loss: 76.0667 - val_loss: 49.2054\n",
      "Epoch 302/700\n",
      " - 21s - loss: 76.0413 - val_loss: 47.4068\n",
      "Epoch 303/700\n",
      " - 21s - loss: 76.1167 - val_loss: 53.4576\n",
      "Epoch 304/700\n",
      " - 21s - loss: 75.9007 - val_loss: 50.8725\n",
      "Epoch 305/700\n",
      " - 21s - loss: 75.8257 - val_loss: 51.8221\n",
      "Epoch 306/700\n",
      " - 21s - loss: 75.9973 - val_loss: 50.2461\n",
      "Epoch 307/700\n",
      " - 21s - loss: 76.0359 - val_loss: 51.9944\n",
      "Epoch 308/700\n",
      " - 21s - loss: 76.5726 - val_loss: 51.9518\n",
      "Epoch 309/700\n",
      " - 21s - loss: 76.1352 - val_loss: 55.7731\n",
      "Epoch 310/700\n",
      " - 21s - loss: 75.9492 - val_loss: 56.1705\n",
      "Epoch 311/700\n",
      " - 21s - loss: 76.6148 - val_loss: 47.4911\n",
      "Epoch 312/700\n",
      " - 21s - loss: 76.2768 - val_loss: 49.9926\n",
      "Epoch 313/700\n",
      " - 21s - loss: 76.6691 - val_loss: 56.7468\n",
      "Epoch 314/700\n",
      " - 21s - loss: 76.7650 - val_loss: 54.3445\n",
      "Epoch 315/700\n",
      " - 21s - loss: 76.7087 - val_loss: 50.4327\n",
      "Epoch 316/700\n",
      " - 21s - loss: 77.7451 - val_loss: 45.4249\n",
      "Epoch 317/700\n",
      " - 21s - loss: 78.7862 - val_loss: 46.3446\n",
      "Epoch 318/700\n",
      " - 21s - loss: 77.2180 - val_loss: 50.5771\n",
      "Epoch 319/700\n",
      " - 21s - loss: 77.0890 - val_loss: 65.6166\n",
      "Epoch 320/700\n",
      " - 21s - loss: 77.5769 - val_loss: 57.7857\n",
      "Epoch 321/700\n",
      " - 21s - loss: 77.0553 - val_loss: 51.1525\n",
      "Epoch 322/700\n",
      " - 21s - loss: 75.9149 - val_loss: 50.7158\n",
      "Epoch 323/700\n",
      " - 21s - loss: 75.8261 - val_loss: 52.1800\n",
      "Epoch 324/700\n",
      " - 21s - loss: 78.0692 - val_loss: 59.6199\n",
      "Epoch 325/700\n",
      " - 21s - loss: 77.0969 - val_loss: 60.3935\n",
      "Epoch 326/700\n",
      " - 21s - loss: 76.3741 - val_loss: 55.4493\n",
      "Epoch 327/700\n",
      " - 21s - loss: 76.1491 - val_loss: 52.4879\n",
      "Epoch 328/700\n",
      " - 21s - loss: 75.7286 - val_loss: 48.4496\n",
      "Epoch 329/700\n",
      " - 21s - loss: 76.3712 - val_loss: 49.3412\n",
      "Epoch 330/700\n",
      " - 21s - loss: 75.7011 - val_loss: 48.5975\n",
      "Epoch 331/700\n",
      " - 21s - loss: 75.7033 - val_loss: 47.0070\n",
      "Epoch 332/700\n",
      " - 21s - loss: 76.1467 - val_loss: 49.2585\n",
      "Epoch 333/700\n",
      " - 21s - loss: 75.6415 - val_loss: 51.9993\n",
      "Epoch 334/700\n",
      " - 21s - loss: 75.6233 - val_loss: 50.1647\n",
      "Epoch 335/700\n",
      " - 21s - loss: 75.8389 - val_loss: 50.1316\n",
      "Epoch 336/700\n",
      " - 21s - loss: 75.7950 - val_loss: 49.7615\n",
      "Epoch 337/700\n",
      " - 21s - loss: 76.0549 - val_loss: 54.2587\n",
      "Epoch 338/700\n",
      " - 21s - loss: 76.3033 - val_loss: 54.8438\n",
      "Epoch 339/700\n",
      " - 21s - loss: 75.8519 - val_loss: 63.5620\n",
      "Epoch 340/700\n",
      " - 21s - loss: 78.5689 - val_loss: 52.0622\n",
      "Epoch 341/700\n",
      " - 21s - loss: 77.8500 - val_loss: 47.3093\n",
      "Epoch 342/700\n",
      " - 21s - loss: 75.5746 - val_loss: 54.9024\n",
      "Epoch 343/700\n",
      " - 21s - loss: 75.8573 - val_loss: 56.3718\n",
      "Epoch 344/700\n",
      " - 21s - loss: 75.6345 - val_loss: 45.7182\n",
      "Epoch 345/700\n",
      " - 21s - loss: 77.2562 - val_loss: 50.8797\n",
      "Epoch 346/700\n",
      " - 21s - loss: 76.2704 - val_loss: 56.4486\n",
      "Epoch 347/700\n",
      " - 21s - loss: 75.8108 - val_loss: 50.5410\n",
      "Epoch 348/700\n",
      " - 21s - loss: 75.5829 - val_loss: 50.5258\n",
      "Epoch 349/700\n",
      " - 21s - loss: 75.3059 - val_loss: 51.4337\n",
      "Epoch 350/700\n",
      " - 21s - loss: 75.8314 - val_loss: 53.5690\n",
      "Epoch 351/700\n",
      " - 21s - loss: 75.6762 - val_loss: 47.5316\n",
      "Epoch 352/700\n",
      " - 21s - loss: 76.5442 - val_loss: 52.0989\n",
      "Epoch 353/700\n",
      " - 21s - loss: 77.1615 - val_loss: 67.6242\n",
      "Epoch 354/700\n",
      " - 21s - loss: 79.3103 - val_loss: 48.2934\n",
      "Epoch 355/700\n",
      " - 21s - loss: 76.5957 - val_loss: 48.4159\n",
      "Epoch 356/700\n",
      " - 21s - loss: 75.7505 - val_loss: 50.7175\n",
      "Epoch 357/700\n",
      " - 21s - loss: 75.4811 - val_loss: 56.2477\n",
      "Epoch 358/700\n",
      " - 21s - loss: 75.5263 - val_loss: 57.2453\n",
      "Epoch 359/700\n",
      " - 21s - loss: 75.8881 - val_loss: 61.6208\n",
      "Epoch 360/700\n",
      " - 21s - loss: 76.8382 - val_loss: 47.2280\n",
      "Epoch 361/700\n",
      " - 21s - loss: 76.7556 - val_loss: 48.3113\n",
      "Epoch 362/700\n",
      " - 21s - loss: 75.5788 - val_loss: 54.0852\n",
      "Epoch 363/700\n",
      " - 21s - loss: 75.5885 - val_loss: 46.3933\n",
      "Epoch 364/700\n",
      " - 21s - loss: 75.9077 - val_loss: 46.8523\n",
      "Epoch 365/700\n",
      " - 21s - loss: 75.6294 - val_loss: 47.4860\n",
      "Epoch 366/700\n",
      " - 21s - loss: 75.4630 - val_loss: 53.1092\n",
      "Epoch 367/700\n",
      " - 21s - loss: 76.7812 - val_loss: 54.7138\n",
      "Epoch 368/700\n",
      " - 21s - loss: 76.0187 - val_loss: 48.9371\n",
      "Epoch 369/700\n",
      " - 21s - loss: 75.7941 - val_loss: 48.2905\n",
      "Epoch 370/700\n",
      " - 21s - loss: 75.6104 - val_loss: 52.8421\n",
      "Epoch 371/700\n",
      " - 21s - loss: 75.2161 - val_loss: 53.3106\n",
      "Epoch 372/700\n",
      " - 21s - loss: 75.3297 - val_loss: 55.0351\n",
      "Epoch 373/700\n",
      " - 21s - loss: 75.5034 - val_loss: 64.1928\n",
      "Epoch 374/700\n",
      " - 21s - loss: 77.3231 - val_loss: 47.6871\n",
      "Epoch 375/700\n",
      " - 21s - loss: 76.0696 - val_loss: 46.2503\n",
      "Epoch 376/700\n",
      " - 21s - loss: 75.7070 - val_loss: 52.4254\n",
      "Epoch 377/700\n",
      " - 21s - loss: 75.5115 - val_loss: 53.9435\n",
      "Epoch 378/700\n",
      " - 21s - loss: 75.4011 - val_loss: 59.2557\n",
      "Epoch 379/700\n",
      " - 21s - loss: 75.7468 - val_loss: 53.0184\n",
      "Epoch 380/700\n",
      " - 21s - loss: 75.3569 - val_loss: 59.4961\n",
      "Epoch 381/700\n",
      " - 21s - loss: 75.6749 - val_loss: 50.2774\n",
      "Epoch 382/700\n",
      " - 21s - loss: 75.3813 - val_loss: 52.2491\n",
      "Epoch 383/700\n",
      " - 21s - loss: 76.3531 - val_loss: 46.2314\n",
      "Epoch 384/700\n",
      " - 21s - loss: 75.5967 - val_loss: 52.1024\n",
      "Epoch 385/700\n",
      " - 21s - loss: 75.4477 - val_loss: 48.4443\n",
      "Epoch 386/700\n",
      " - 21s - loss: 75.0333 - val_loss: 53.4838\n",
      "Epoch 387/700\n",
      " - 21s - loss: 74.9674 - val_loss: 55.6273\n",
      "Epoch 388/700\n",
      " - 21s - loss: 76.2645 - val_loss: 58.7592\n",
      "Epoch 389/700\n",
      " - 21s - loss: 75.9554 - val_loss: 46.6344\n",
      "Epoch 390/700\n",
      " - 21s - loss: 76.2355 - val_loss: 49.1371\n",
      "Epoch 391/700\n",
      " - 21s - loss: 77.3855 - val_loss: 62.9183\n",
      "Epoch 392/700\n",
      " - 21s - loss: 77.2008 - val_loss: 60.3617\n",
      "Epoch 393/700\n",
      " - 21s - loss: 75.6096 - val_loss: 48.2427\n",
      "Epoch 394/700\n",
      " - 21s - loss: 75.7676 - val_loss: 47.7362\n",
      "Epoch 395/700\n",
      " - 21s - loss: 76.0869 - val_loss: 62.8471\n",
      "Epoch 396/700\n",
      " - 21s - loss: 75.8428 - val_loss: 56.3823\n",
      "Epoch 397/700\n",
      " - 21s - loss: 77.0694 - val_loss: 45.4931\n",
      "Epoch 398/700\n",
      " - 21s - loss: 78.3887 - val_loss: 48.1937\n",
      "Epoch 399/700\n",
      " - 21s - loss: 75.7885 - val_loss: 55.8307\n",
      "Epoch 400/700\n",
      " - 21s - loss: 76.0590 - val_loss: 56.6081\n",
      "Epoch 401/700\n",
      " - 21s - loss: 75.6425 - val_loss: 47.3629\n",
      "Epoch 402/700\n",
      " - 21s - loss: 75.6317 - val_loss: 60.8387\n",
      "Epoch 403/700\n",
      " - 21s - loss: 77.5331 - val_loss: 57.6566\n",
      "Epoch 404/700\n",
      " - 21s - loss: 75.9427 - val_loss: 47.1576\n",
      "Epoch 405/700\n",
      " - 21s - loss: 75.5589 - val_loss: 54.1633\n",
      "Epoch 406/700\n",
      " - 21s - loss: 74.9236 - val_loss: 52.4102\n",
      "Epoch 407/700\n",
      " - 21s - loss: 74.9982 - val_loss: 49.3135\n",
      "Epoch 408/700\n",
      " - 21s - loss: 75.4738 - val_loss: 48.7541\n",
      "Epoch 409/700\n",
      " - 21s - loss: 75.4866 - val_loss: 57.9371\n",
      "Epoch 410/700\n",
      " - 21s - loss: 76.2909 - val_loss: 54.7585\n",
      "Epoch 411/700\n",
      " - 21s - loss: 75.0355 - val_loss: 50.6349\n",
      "Epoch 412/700\n",
      " - 21s - loss: 75.2659 - val_loss: 45.5619\n",
      "Epoch 413/700\n",
      " - 21s - loss: 76.3993 - val_loss: 46.7614\n",
      "Epoch 414/700\n",
      " - 21s - loss: 75.4781 - val_loss: 50.1950\n",
      "Epoch 415/700\n",
      " - 21s - loss: 74.8388 - val_loss: 53.0631\n",
      "Epoch 416/700\n",
      " - 21s - loss: 75.1049 - val_loss: 54.6010\n",
      "Epoch 417/700\n",
      " - 21s - loss: 74.8260 - val_loss: 48.9705\n",
      "Epoch 418/700\n",
      " - 21s - loss: 74.9664 - val_loss: 49.4218\n",
      "Epoch 419/700\n",
      " - 21s - loss: 74.8641 - val_loss: 47.9396\n",
      "Epoch 420/700\n",
      " - 21s - loss: 75.9147 - val_loss: 63.6446\n",
      "Epoch 421/700\n",
      " - 21s - loss: 79.0961 - val_loss: 50.0288\n",
      "Epoch 422/700\n",
      " - 21s - loss: 77.6972 - val_loss: 46.4210\n",
      "Epoch 423/700\n",
      " - 21s - loss: 75.7319 - val_loss: 46.5509\n",
      "Epoch 424/700\n",
      " - 21s - loss: 75.6531 - val_loss: 52.1465\n",
      "Epoch 425/700\n",
      " - 21s - loss: 74.7872 - val_loss: 55.1587\n",
      "Epoch 426/700\n",
      " - 21s - loss: 75.0597 - val_loss: 56.2707\n",
      "Epoch 427/700\n",
      " - 21s - loss: 75.0791 - val_loss: 52.5576\n",
      "Epoch 428/700\n",
      " - 21s - loss: 74.8393 - val_loss: 54.0591\n",
      "Epoch 429/700\n",
      " - 21s - loss: 75.1130 - val_loss: 48.1171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/700\n",
      " - 21s - loss: 74.7271 - val_loss: 54.5111\n",
      "Epoch 431/700\n",
      " - 21s - loss: 75.0675 - val_loss: 50.5861\n",
      "Epoch 432/700\n",
      " - 21s - loss: 74.7335 - val_loss: 47.9814\n",
      "Epoch 433/700\n",
      " - 21s - loss: 74.7854 - val_loss: 47.7696\n",
      "Epoch 434/700\n",
      " - 21s - loss: 75.1552 - val_loss: 54.4907\n",
      "Epoch 435/700\n",
      " - 21s - loss: 75.8167 - val_loss: 52.4561\n",
      "Epoch 436/700\n",
      " - 21s - loss: 75.7754 - val_loss: 46.3332\n",
      "Epoch 437/700\n",
      " - 21s - loss: 75.3220 - val_loss: 52.6952\n",
      "Epoch 438/700\n",
      " - 21s - loss: 74.5566 - val_loss: 54.4781\n",
      "Epoch 439/700\n",
      " - 21s - loss: 75.0435 - val_loss: 51.9074\n",
      "Epoch 440/700\n",
      " - 21s - loss: 74.4568 - val_loss: 49.8369\n",
      "Epoch 441/700\n",
      " - 21s - loss: 75.0604 - val_loss: 53.9276\n",
      "Epoch 442/700\n",
      " - 21s - loss: 75.8793 - val_loss: 59.9342\n",
      "Epoch 443/700\n",
      " - 21s - loss: 74.8279 - val_loss: 50.7102\n",
      "Epoch 444/700\n",
      " - 21s - loss: 74.6342 - val_loss: 47.7391\n",
      "Epoch 445/700\n",
      " - 21s - loss: 75.3950 - val_loss: 54.8177\n",
      "Epoch 446/700\n",
      " - 21s - loss: 75.4450 - val_loss: 48.8494\n",
      "Epoch 447/700\n",
      " - 21s - loss: 76.4978 - val_loss: 46.5820\n",
      "Epoch 448/700\n",
      " - 21s - loss: 75.0499 - val_loss: 54.0193\n",
      "Epoch 449/700\n",
      " - 21s - loss: 75.2164 - val_loss: 52.3386\n",
      "Epoch 450/700\n",
      " - 21s - loss: 75.5611 - val_loss: 48.0675\n",
      "Epoch 451/700\n",
      " - 21s - loss: 75.5688 - val_loss: 45.8469\n",
      "Epoch 452/700\n",
      " - 21s - loss: 76.2862 - val_loss: 61.4732\n",
      "Epoch 453/700\n",
      " - 21s - loss: 76.9292 - val_loss: 59.2611\n",
      "Epoch 454/700\n",
      " - 21s - loss: 75.9555 - val_loss: 48.9668\n",
      "Epoch 455/700\n",
      " - 21s - loss: 75.1160 - val_loss: 47.8851\n",
      "Epoch 456/700\n",
      " - 21s - loss: 75.3901 - val_loss: 53.3546\n",
      "Epoch 457/700\n",
      " - 21s - loss: 75.7483 - val_loss: 45.8592\n",
      "Epoch 458/700\n",
      " - 21s - loss: 76.9244 - val_loss: 52.6628\n",
      "Epoch 459/700\n",
      " - 21s - loss: 74.7980 - val_loss: 55.1907\n",
      "Epoch 460/700\n",
      " - 21s - loss: 74.8410 - val_loss: 56.5234\n",
      "Epoch 461/700\n",
      " - 21s - loss: 74.9377 - val_loss: 49.4835\n",
      "Epoch 462/700\n",
      " - 21s - loss: 75.0517 - val_loss: 46.8918\n",
      "Epoch 463/700\n",
      " - 21s - loss: 76.0098 - val_loss: 63.6487\n",
      "Epoch 464/700\n",
      " - 21s - loss: 79.3316 - val_loss: 60.9659\n",
      "Epoch 465/700\n",
      " - 21s - loss: 77.0448 - val_loss: 46.8016\n",
      "Epoch 466/700\n",
      " - 21s - loss: 76.3452 - val_loss: 51.6081\n",
      "Epoch 467/700\n",
      " - 21s - loss: 75.5950 - val_loss: 56.9409\n",
      "Epoch 468/700\n",
      " - 21s - loss: 74.5138 - val_loss: 57.3354\n",
      "Epoch 469/700\n",
      " - 21s - loss: 75.1548 - val_loss: 48.5047\n",
      "Epoch 470/700\n",
      " - 21s - loss: 74.5512 - val_loss: 59.3680\n",
      "Epoch 471/700\n",
      " - 21s - loss: 74.8194 - val_loss: 50.8657\n",
      "Epoch 472/700\n",
      " - 21s - loss: 74.5004 - val_loss: 48.9428\n",
      "Epoch 473/700\n",
      " - 21s - loss: 74.4698 - val_loss: 49.1892\n",
      "Epoch 474/700\n",
      " - 21s - loss: 74.3288 - val_loss: 58.5795\n",
      "Epoch 475/700\n",
      " - 21s - loss: 75.9437 - val_loss: 47.6355\n",
      "Epoch 476/700\n",
      " - 21s - loss: 78.3187 - val_loss: 45.9383\n",
      "Epoch 477/700\n",
      " - 21s - loss: 77.9124 - val_loss: 58.1316\n",
      "Epoch 478/700\n",
      " - 21s - loss: 75.7683 - val_loss: 54.8756\n",
      "Epoch 479/700\n",
      " - 21s - loss: 76.3007 - val_loss: 45.9290\n",
      "Epoch 480/700\n",
      " - 21s - loss: 77.1003 - val_loss: 46.2938\n",
      "Epoch 481/700\n",
      " - 21s - loss: 76.1714 - val_loss: 55.2123\n",
      "Epoch 482/700\n",
      " - 21s - loss: 75.0674 - val_loss: 60.8604\n",
      "Epoch 483/700\n",
      " - 21s - loss: 75.6380 - val_loss: 48.3539\n",
      "Epoch 484/700\n",
      " - 21s - loss: 74.9674 - val_loss: 45.7631\n",
      "Epoch 485/700\n",
      " - 21s - loss: 76.7773 - val_loss: 56.1488\n",
      "Epoch 486/700\n",
      " - 21s - loss: 77.9231 - val_loss: 68.9191\n",
      "Epoch 487/700\n",
      " - 21s - loss: 79.1006 - val_loss: 46.0020\n",
      "Epoch 488/700\n",
      " - 21s - loss: 78.6180 - val_loss: 46.7716\n",
      "Epoch 489/700\n",
      " - 21s - loss: 76.3824 - val_loss: 58.4057\n",
      "Epoch 490/700\n",
      " - 21s - loss: 77.2703 - val_loss: 68.3247\n",
      "Epoch 491/700\n",
      " - 21s - loss: 76.3462 - val_loss: 49.3545\n",
      "Epoch 492/700\n",
      " - 21s - loss: 76.1442 - val_loss: 45.8325\n",
      "Epoch 493/700\n",
      " - 21s - loss: 78.1279 - val_loss: 52.6879\n",
      "Epoch 494/700\n",
      " - 21s - loss: 76.1793 - val_loss: 59.3872\n",
      "Epoch 495/700\n",
      " - 21s - loss: 75.6898 - val_loss: 56.2068\n",
      "Epoch 496/700\n",
      " - 21s - loss: 75.9746 - val_loss: 47.2132\n",
      "Epoch 497/700\n",
      " - 21s - loss: 75.5179 - val_loss: 49.2608\n",
      "Epoch 498/700\n",
      " - 21s - loss: 75.1336 - val_loss: 51.9325\n",
      "Epoch 499/700\n",
      " - 21s - loss: 74.6450 - val_loss: 51.3292\n",
      "Epoch 500/700\n",
      " - 21s - loss: 74.0868 - val_loss: 59.9727\n",
      "Epoch 501/700\n",
      " - 21s - loss: 74.7004 - val_loss: 58.1029\n",
      "Epoch 502/700\n",
      " - 21s - loss: 75.2490 - val_loss: 52.8575\n",
      "Epoch 503/700\n",
      " - 21s - loss: 74.3675 - val_loss: 52.9622\n",
      "Epoch 504/700\n",
      " - 21s - loss: 75.2945 - val_loss: 47.1301\n",
      "Epoch 505/700\n",
      " - 21s - loss: 76.4355 - val_loss: 59.1279\n",
      "Epoch 506/700\n",
      " - 21s - loss: 76.3526 - val_loss: 61.2166\n",
      "Epoch 507/700\n",
      " - 21s - loss: 78.3216 - val_loss: 46.4598\n",
      "Epoch 508/700\n",
      " - 21s - loss: 78.8119 - val_loss: 46.4406\n",
      "Epoch 509/700\n",
      " - 21s - loss: 74.6531 - val_loss: 50.6812\n",
      "Epoch 510/700\n",
      " - 21s - loss: 74.1646 - val_loss: 52.2170\n",
      "Epoch 511/700\n",
      " - 21s - loss: 74.6840 - val_loss: 49.7412\n",
      "Epoch 512/700\n",
      " - 21s - loss: 74.1362 - val_loss: 48.1114\n",
      "Epoch 513/700\n",
      " - 21s - loss: 74.2852 - val_loss: 49.3361\n",
      "Epoch 514/700\n",
      " - 21s - loss: 74.3523 - val_loss: 47.7536\n",
      "Epoch 515/700\n",
      " - 21s - loss: 74.0813 - val_loss: 51.4963\n",
      "Epoch 516/700\n",
      " - 21s - loss: 74.3507 - val_loss: 56.0361\n",
      "Epoch 517/700\n",
      " - 21s - loss: 74.6229 - val_loss: 52.2289\n",
      "Epoch 518/700\n",
      " - 21s - loss: 74.3576 - val_loss: 49.3268\n",
      "Epoch 519/700\n",
      " - 21s - loss: 74.6437 - val_loss: 46.0759\n",
      "Epoch 520/700\n",
      " - 21s - loss: 75.7654 - val_loss: 48.0811\n",
      "Epoch 521/700\n",
      " - 21s - loss: 74.2952 - val_loss: 52.7810\n",
      "Epoch 522/700\n",
      " - 21s - loss: 74.1564 - val_loss: 55.7350\n",
      "Epoch 523/700\n",
      " - 21s - loss: 74.2283 - val_loss: 50.5636\n",
      "Epoch 524/700\n",
      " - 21s - loss: 74.1469 - val_loss: 48.3742\n",
      "Epoch 525/700\n",
      " - 21s - loss: 74.3628 - val_loss: 47.3940\n",
      "Epoch 526/700\n",
      " - 21s - loss: 74.2754 - val_loss: 51.7439\n",
      "Epoch 527/700\n",
      " - 21s - loss: 73.8771 - val_loss: 46.9035\n",
      "Epoch 528/700\n",
      " - 21s - loss: 74.9436 - val_loss: 52.0081\n",
      "Epoch 529/700\n",
      " - 21s - loss: 76.1632 - val_loss: 53.3955\n",
      "Epoch 530/700\n",
      " - 21s - loss: 74.1212 - val_loss: 48.5376\n",
      "Epoch 531/700\n",
      " - 21s - loss: 74.3358 - val_loss: 52.6290\n",
      "Epoch 532/700\n",
      " - 21s - loss: 74.0551 - val_loss: 47.6850\n",
      "Epoch 533/700\n",
      " - 21s - loss: 74.9994 - val_loss: 58.2766\n",
      "Epoch 534/700\n",
      " - 21s - loss: 75.6566 - val_loss: 48.5122\n",
      "Epoch 535/700\n",
      " - 21s - loss: 76.1083 - val_loss: 46.8508\n",
      "Epoch 536/700\n",
      " - 21s - loss: 76.0606 - val_loss: 59.1467\n",
      "Epoch 537/700\n",
      " - 21s - loss: 76.2943 - val_loss: 51.5535\n",
      "Epoch 538/700\n",
      " - 21s - loss: 74.4465 - val_loss: 47.9201\n",
      "Epoch 539/700\n",
      " - 21s - loss: 74.1230 - val_loss: 50.1857\n",
      "Epoch 540/700\n",
      " - 21s - loss: 74.0743 - val_loss: 57.3886\n",
      "Epoch 541/700\n",
      " - 21s - loss: 75.1251 - val_loss: 47.4893\n",
      "Epoch 542/700\n",
      " - 21s - loss: 75.0983 - val_loss: 47.9534\n",
      "Epoch 543/700\n",
      " - 21s - loss: 75.0153 - val_loss: 56.5650\n",
      "Epoch 544/700\n",
      " - 21s - loss: 75.0607 - val_loss: 50.2931\n",
      "Epoch 545/700\n",
      " - 21s - loss: 73.8976 - val_loss: 54.2108\n",
      "Epoch 546/700\n",
      " - 21s - loss: 74.1567 - val_loss: 49.9193\n",
      "Epoch 547/700\n",
      " - 21s - loss: 74.3936 - val_loss: 47.3765\n",
      "Epoch 548/700\n",
      " - 21s - loss: 75.2946 - val_loss: 61.0119\n",
      "Epoch 549/700\n",
      " - 21s - loss: 75.9792 - val_loss: 50.4746\n",
      "Epoch 550/700\n",
      " - 21s - loss: 74.7071 - val_loss: 46.0805\n",
      "Epoch 551/700\n",
      " - 21s - loss: 74.6463 - val_loss: 47.8933\n",
      "Epoch 552/700\n",
      " - 21s - loss: 74.7863 - val_loss: 52.3402\n",
      "Epoch 553/700\n",
      " - 21s - loss: 74.7176 - val_loss: 60.5398\n",
      "Epoch 554/700\n",
      " - 21s - loss: 74.4133 - val_loss: 53.0106\n",
      "Epoch 555/700\n",
      " - 21s - loss: 74.0667 - val_loss: 50.5690\n",
      "Epoch 556/700\n",
      " - 21s - loss: 73.8717 - val_loss: 48.2805\n",
      "Epoch 557/700\n",
      " - 21s - loss: 73.8773 - val_loss: 49.7153\n",
      "Epoch 558/700\n",
      " - 21s - loss: 74.3498 - val_loss: 59.1028\n",
      "Epoch 559/700\n",
      " - 21s - loss: 76.0532 - val_loss: 54.2932\n",
      "Epoch 560/700\n",
      " - 21s - loss: 74.2281 - val_loss: 50.0497\n",
      "Epoch 561/700\n",
      " - 21s - loss: 73.9122 - val_loss: 49.1435\n",
      "Epoch 562/700\n",
      " - 21s - loss: 74.4539 - val_loss: 47.4698\n",
      "Epoch 563/700\n",
      " - 21s - loss: 73.6955 - val_loss: 47.5997\n",
      "Epoch 564/700\n",
      " - 21s - loss: 74.1086 - val_loss: 48.6049\n",
      "Epoch 565/700\n",
      " - 21s - loss: 73.9382 - val_loss: 47.2870\n",
      "Epoch 566/700\n",
      " - 21s - loss: 74.6156 - val_loss: 54.9455\n",
      "Epoch 567/700\n",
      " - 21s - loss: 74.6284 - val_loss: 57.3123\n",
      "Epoch 568/700\n",
      " - 21s - loss: 74.5515 - val_loss: 49.3035\n",
      "Epoch 569/700\n",
      " - 21s - loss: 74.0009 - val_loss: 47.6828\n",
      "Epoch 570/700\n",
      " - 21s - loss: 74.1515 - val_loss: 61.5475\n",
      "Epoch 571/700\n",
      " - 21s - loss: 75.4366 - val_loss: 46.1958\n",
      "Epoch 572/700\n",
      " - 21s - loss: 77.6058 - val_loss: 46.3469\n",
      "Epoch 573/700\n",
      " - 21s - loss: 78.9632 - val_loss: 60.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 574/700\n",
      " - 21s - loss: 74.6894 - val_loss: 52.8373\n",
      "Epoch 575/700\n",
      " - 21s - loss: 73.9548 - val_loss: 46.7977\n",
      "Epoch 576/700\n",
      " - 21s - loss: 74.5216 - val_loss: 50.0947\n",
      "Epoch 577/700\n",
      " - 21s - loss: 73.9293 - val_loss: 55.4294\n",
      "Epoch 578/700\n",
      " - 21s - loss: 74.2811 - val_loss: 51.3503\n",
      "Epoch 579/700\n",
      " - 21s - loss: 73.7593 - val_loss: 47.8535\n",
      "Epoch 580/700\n",
      " - 21s - loss: 73.9197 - val_loss: 51.9811\n",
      "Epoch 581/700\n",
      " - 21s - loss: 73.9724 - val_loss: 58.8205\n",
      "Epoch 582/700\n",
      " - 21s - loss: 75.1276 - val_loss: 54.2888\n",
      "Epoch 583/700\n",
      " - 21s - loss: 74.2486 - val_loss: 48.7904\n",
      "Epoch 584/700\n",
      " - 21s - loss: 74.4890 - val_loss: 47.3097\n",
      "Epoch 585/700\n",
      " - 21s - loss: 74.4955 - val_loss: 64.9761\n",
      "Epoch 586/700\n",
      " - 21s - loss: 75.3930 - val_loss: 49.9223\n",
      "Epoch 587/700\n",
      " - 21s - loss: 73.7595 - val_loss: 47.5297\n",
      "Epoch 588/700\n",
      " - 21s - loss: 75.9734 - val_loss: 46.9443\n",
      "Epoch 589/700\n",
      " - 21s - loss: 75.5225 - val_loss: 64.9961\n",
      "Epoch 590/700\n",
      " - 21s - loss: 75.9571 - val_loss: 50.1516\n",
      "Epoch 591/700\n",
      " - 21s - loss: 74.5785 - val_loss: 46.5645\n",
      "Epoch 592/700\n",
      " - 21s - loss: 75.2747 - val_loss: 57.2823\n",
      "Epoch 593/700\n",
      " - 21s - loss: 74.3292 - val_loss: 57.1766\n",
      "Epoch 594/700\n",
      " - 21s - loss: 73.8354 - val_loss: 51.4177\n",
      "Epoch 595/700\n",
      " - 21s - loss: 73.9291 - val_loss: 48.8154\n",
      "Epoch 596/700\n",
      " - 21s - loss: 74.9394 - val_loss: 65.5262\n",
      "Epoch 597/700\n",
      " - 21s - loss: 75.6193 - val_loss: 49.3442\n",
      "Epoch 598/700\n",
      " - 21s - loss: 74.2278 - val_loss: 49.1265\n",
      "Epoch 599/700\n",
      " - 21s - loss: 74.0963 - val_loss: 48.9637\n",
      "Epoch 600/700\n",
      " - 21s - loss: 74.2561 - val_loss: 47.7451\n",
      "Epoch 601/700\n",
      " - 21s - loss: 74.0055 - val_loss: 49.8352\n",
      "Epoch 602/700\n",
      " - 21s - loss: 73.6803 - val_loss: 49.9785\n",
      "Epoch 603/700\n",
      " - 21s - loss: 73.6927 - val_loss: 58.1403\n",
      "Epoch 604/700\n",
      " - 21s - loss: 73.9262 - val_loss: 55.8727\n",
      "Epoch 605/700\n",
      " - 21s - loss: 74.3608 - val_loss: 48.3637\n",
      "Epoch 606/700\n",
      " - 21s - loss: 75.3210 - val_loss: 46.2535\n",
      "Epoch 607/700\n",
      " - 21s - loss: 76.7205 - val_loss: 57.9698\n",
      "Epoch 608/700\n",
      " - 21s - loss: 77.5994 - val_loss: 53.7814\n",
      "Epoch 609/700\n",
      " - 21s - loss: 75.2508 - val_loss: 46.5240\n",
      "Epoch 610/700\n",
      " - 21s - loss: 74.3976 - val_loss: 52.7383\n",
      "Epoch 611/700\n",
      " - 21s - loss: 74.1793 - val_loss: 56.2381\n",
      "Epoch 612/700\n",
      " - 21s - loss: 74.0725 - val_loss: 50.3989\n",
      "Epoch 613/700\n",
      " - 21s - loss: 73.7814 - val_loss: 51.6852\n",
      "Epoch 614/700\n",
      " - 21s - loss: 73.4977 - val_loss: 48.1994\n",
      "Epoch 615/700\n",
      " - 21s - loss: 74.2499 - val_loss: 56.4490\n",
      "Epoch 616/700\n",
      " - 21s - loss: 74.2011 - val_loss: 51.5280\n",
      "Epoch 617/700\n",
      " - 21s - loss: 73.7390 - val_loss: 49.8960\n",
      "Epoch 618/700\n",
      " - 21s - loss: 74.6839 - val_loss: 62.1723\n",
      "Epoch 619/700\n",
      " - 21s - loss: 75.2922 - val_loss: 50.1904\n",
      "Epoch 620/700\n",
      " - 21s - loss: 74.1924 - val_loss: 50.0988\n",
      "Epoch 621/700\n",
      " - 21s - loss: 73.5120 - val_loss: 49.4955\n",
      "Epoch 622/700\n",
      " - 21s - loss: 74.1976 - val_loss: 48.5564\n",
      "Epoch 623/700\n",
      " - 21s - loss: 74.8313 - val_loss: 65.9049\n",
      "Epoch 624/700\n",
      " - 21s - loss: 77.5150 - val_loss: 49.4622\n",
      "Epoch 625/700\n",
      " - 21s - loss: 77.0911 - val_loss: 46.4426\n",
      "Epoch 626/700\n",
      " - 21s - loss: 74.6504 - val_loss: 50.0829\n",
      "Epoch 627/700\n",
      " - 21s - loss: 74.9362 - val_loss: 61.4875\n",
      "Epoch 628/700\n",
      " - 21s - loss: 74.1178 - val_loss: 49.2344\n",
      "Epoch 629/700\n",
      " - 21s - loss: 73.6761 - val_loss: 50.0211\n",
      "Epoch 630/700\n",
      " - 21s - loss: 73.7808 - val_loss: 47.9445\n",
      "Epoch 631/700\n",
      " - 21s - loss: 73.9022 - val_loss: 49.8547\n",
      "Epoch 632/700\n",
      " - 21s - loss: 74.3466 - val_loss: 54.0523\n",
      "Epoch 633/700\n",
      " - 21s - loss: 73.5790 - val_loss: 50.8500\n",
      "Epoch 634/700\n",
      " - 21s - loss: 73.5155 - val_loss: 55.8505\n",
      "Epoch 635/700\n",
      " - 21s - loss: 74.3782 - val_loss: 49.0828\n",
      "Epoch 636/700\n",
      " - 21s - loss: 73.4583 - val_loss: 53.3036\n",
      "Epoch 637/700\n",
      " - 21s - loss: 73.4953 - val_loss: 47.7980\n",
      "Epoch 638/700\n",
      " - 21s - loss: 73.7018 - val_loss: 48.9188\n",
      "Epoch 639/700\n",
      " - 21s - loss: 75.1901 - val_loss: 46.7243\n",
      "Epoch 640/700\n",
      " - 21s - loss: 74.4364 - val_loss: 48.3967\n",
      "Epoch 641/700\n",
      " - 21s - loss: 73.9519 - val_loss: 52.9928\n",
      "Epoch 642/700\n",
      " - 21s - loss: 73.7641 - val_loss: 52.3358\n",
      "Epoch 643/700\n",
      " - 21s - loss: 73.7009 - val_loss: 49.1747\n",
      "Epoch 644/700\n",
      " - 21s - loss: 73.3943 - val_loss: 51.0207\n",
      "Epoch 645/700\n",
      " - 21s - loss: 74.1503 - val_loss: 48.4671\n",
      "Epoch 646/700\n",
      " - 21s - loss: 74.3233 - val_loss: 53.2373\n",
      "Epoch 647/700\n",
      " - 21s - loss: 73.7033 - val_loss: 51.5565\n",
      "Epoch 648/700\n",
      " - 21s - loss: 73.4202 - val_loss: 51.8553\n",
      "Epoch 649/700\n",
      " - 21s - loss: 73.2943 - val_loss: 53.7772\n",
      "Epoch 650/700\n",
      " - 21s - loss: 73.7977 - val_loss: 49.7668\n",
      "Epoch 651/700\n",
      " - 21s - loss: 73.8531 - val_loss: 57.5803\n",
      "Epoch 652/700\n",
      " - 21s - loss: 73.4106 - val_loss: 51.1017\n",
      "Epoch 653/700\n",
      " - 21s - loss: 73.2479 - val_loss: 47.8749\n",
      "Epoch 654/700\n",
      " - 21s - loss: 73.8249 - val_loss: 52.1136\n",
      "Epoch 655/700\n",
      " - 21s - loss: 73.4788 - val_loss: 49.7768\n",
      "Epoch 656/700\n",
      " - 21s - loss: 74.7932 - val_loss: 51.3161\n",
      "Epoch 657/700\n",
      " - 21s - loss: 73.7490 - val_loss: 49.2469\n",
      "Epoch 658/700\n",
      " - 21s - loss: 74.7091 - val_loss: 49.1480\n",
      "Epoch 659/700\n",
      " - 21s - loss: 75.5472 - val_loss: 61.3815\n",
      "Epoch 660/700\n",
      " - 21s - loss: 74.8247 - val_loss: 46.5757\n",
      "Epoch 661/700\n",
      " - 21s - loss: 74.9665 - val_loss: 50.2719\n",
      "Epoch 662/700\n",
      " - 21s - loss: 73.9563 - val_loss: 60.6094\n",
      "Epoch 663/700\n",
      " - 21s - loss: 74.1136 - val_loss: 47.5541\n",
      "Epoch 664/700\n",
      " - 21s - loss: 75.5030 - val_loss: 47.7621\n",
      "Epoch 665/700\n",
      " - 21s - loss: 75.1656 - val_loss: 62.2305\n",
      "Epoch 666/700\n",
      " - 21s - loss: 75.6095 - val_loss: 46.6372\n",
      "Epoch 667/700\n",
      " - 21s - loss: 79.5879 - val_loss: 47.5973\n",
      "Epoch 668/700\n",
      " - 21s - loss: 78.2394 - val_loss: 75.6793\n",
      "Epoch 669/700\n",
      " - 21s - loss: 77.8547 - val_loss: 48.3265\n",
      "Epoch 670/700\n",
      " - 21s - loss: 74.9220 - val_loss: 47.2679\n",
      "Epoch 671/700\n",
      " - 21s - loss: 74.7322 - val_loss: 60.6623\n",
      "Epoch 672/700\n",
      " - 21s - loss: 73.8892 - val_loss: 50.6074\n",
      "Epoch 673/700\n",
      " - 21s - loss: 74.1171 - val_loss: 48.0737\n",
      "Epoch 674/700\n",
      " - 21s - loss: 73.4112 - val_loss: 49.9078\n",
      "Epoch 675/700\n",
      " - 21s - loss: 73.4259 - val_loss: 56.8322\n",
      "Epoch 676/700\n",
      " - 21s - loss: 73.8514 - val_loss: 48.1274\n",
      "Epoch 677/700\n",
      " - 21s - loss: 73.6875 - val_loss: 51.6754\n",
      "Epoch 678/700\n",
      " - 21s - loss: 73.9600 - val_loss: 48.0755\n",
      "Epoch 679/700\n",
      " - 21s - loss: 73.4708 - val_loss: 48.6384\n",
      "Epoch 680/700\n",
      " - 21s - loss: 73.5149 - val_loss: 49.3405\n",
      "Epoch 681/700\n",
      " - 21s - loss: 73.2132 - val_loss: 60.7651\n",
      "Epoch 682/700\n",
      " - 21s - loss: 74.1782 - val_loss: 53.2043\n",
      "Epoch 683/700\n",
      " - 21s - loss: 73.3912 - val_loss: 49.6907\n",
      "Epoch 684/700\n",
      " - 21s - loss: 74.1564 - val_loss: 49.6360\n",
      "Epoch 685/700\n",
      " - 21s - loss: 74.8228 - val_loss: 54.0850\n",
      "Epoch 686/700\n",
      " - 21s - loss: 73.4852 - val_loss: 51.4901\n",
      "Epoch 687/700\n",
      " - 21s - loss: 73.7611 - val_loss: 49.0082\n",
      "Epoch 688/700\n",
      " - 21s - loss: 73.7944 - val_loss: 48.2150\n",
      "Epoch 689/700\n",
      " - 21s - loss: 73.8811 - val_loss: 48.9213\n",
      "Epoch 690/700\n",
      " - 21s - loss: 73.5274 - val_loss: 55.0203\n",
      "Epoch 691/700\n",
      " - 21s - loss: 74.8441 - val_loss: 55.6788\n",
      "Epoch 692/700\n",
      " - 21s - loss: 73.9084 - val_loss: 50.3479\n",
      "Epoch 693/700\n",
      " - 21s - loss: 74.0652 - val_loss: 58.2839\n",
      "Epoch 694/700\n",
      " - 21s - loss: 74.1274 - val_loss: 51.9391\n",
      "Epoch 695/700\n",
      " - 21s - loss: 73.7174 - val_loss: 48.8137\n",
      "Epoch 696/700\n",
      " - 21s - loss: 73.7858 - val_loss: 56.7777\n",
      "Epoch 697/700\n",
      " - 21s - loss: 73.2997 - val_loss: 52.7473\n",
      "Epoch 698/700\n",
      " - 21s - loss: 73.8429 - val_loss: 48.2968\n",
      "Epoch 699/700\n",
      " - 21s - loss: 73.3205 - val_loss: 51.4220\n",
      "Epoch 700/700\n",
      " - 21s - loss: 73.6556 - val_loss: 57.5323\n"
     ]
    }
   ],
   "source": [
    "modelConvLSTM = Sequential()\n",
    "modelConvLSTM.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', return_sequences=True,input_shape=(X_train.shape[1], 1, X_train.shape[3], X_train.shape[4])))\n",
    "modelConvLSTM.add(ConvLSTM2D(filters=32, kernel_size=(1,2), activation='relu'))\n",
    "modelConvLSTM.add(Flatten())\n",
    "modelConvLSTM.add(Dense(64))\n",
    "modelConvLSTM.add(Dense(32))\n",
    "modelConvLSTM.add(Dense(1))\n",
    "modelConvLSTM.compile(optimizer='adam', loss='mse')\n",
    "history = modelConvLSTM.fit(X_train, y_train, batch_size=512, epochs=700, verbose=2, callbacks=[early_stop], validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Explained Variance Score': 0.6445103495323207,\n",
       " 'Max Error': 39.666272277832036,\n",
       " 'Mean Absolute Error': 6.176845682866156,\n",
       " 'Mean Squared Error': 57.53231258193506,\n",
       " 'Root Mean Squared Error': 7.585005773362012,\n",
       " 'Median Absolute Error': 5.680847320556644,\n",
       " 'RÂ² Score': 0.5632014342417464,\n",
       " 'Mean Absolute Percentage Error': 6.049541816471797}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvLSTMpred = modelConvLSTM.predict(X_valid, verbose=0)\n",
    "ConvLSTMpred = ConvLSTMpred.reshape((ConvLSTMpred.shape[0]))\n",
    "ConvLSTMresults = metrics(ConvLSTMpred, y_valid)\n",
    "ConvLSTMresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvLSTMpred = modelConvLSTM.predict(X_test, verbose=0)\n",
    "ConvLSTMpred = ConvLSTMpred.reshape((ConvLSTMpred.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77.78055,  93.80936,  77.5048 , ...,  93.32718, 111.14896,\n",
       "        95.83975], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvLSTMpred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
