{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, TimeDistributed, ConvLSTM2D, Reshape\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as sm\n",
    "import keras\n",
    "from keras.losses import huber_loss\n",
    "\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n",
       "0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n",
       "1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n",
       "2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n",
       "3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n",
       "4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n",
       "\n",
       "   X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     1     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 378 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  data.drop([\"ID\",\"X0\",\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X8\" ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y  X10  X11  X12  X13  X14  X15  X16  X17  X18  ...  X375  X376  X377  \\\n",
       "0  130.81    0    0    0    1    0    0    0    0    1  ...     0     0     1   \n",
       "1   88.53    0    0    0    0    0    0    0    0    1  ...     1     0     0   \n",
       "2   76.26    0    0    0    0    0    0    0    1    0  ...     0     0     0   \n",
       "3   80.62    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "4   78.02    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "\n",
       "   X378  X379  X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0  \n",
       "2     0     0     0     1     0     0     0  \n",
       "3     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 369)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "      <td>ai</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "      <td>g</td>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>j</td>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>az</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>z</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>w</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>i</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>8410</td>\n",
       "      <td>aj</td>\n",
       "      <td>h</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>j</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>8411</td>\n",
       "      <td>t</td>\n",
       "      <td>aa</td>\n",
       "      <td>ai</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>j</td>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>8413</td>\n",
       "      <td>y</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>8414</td>\n",
       "      <td>ak</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>c</td>\n",
       "      <td>q</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>8416</td>\n",
       "      <td>t</td>\n",
       "      <td>aa</td>\n",
       "      <td>ai</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>r</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows Ã— 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  X0  X1  X2 X3 X4  X5 X6 X8  X10  ...  X375  X376  X377  X378  \\\n",
       "0        1  az   v   n  f  d   t  a  w    0  ...     0     0     0     1   \n",
       "1        2   t   b  ai  a  d   b  g  y    0  ...     0     0     1     0   \n",
       "2        3  az   v  as  f  d   a  j  j    0  ...     0     0     0     1   \n",
       "3        4  az   l   n  f  d   z  l  n    0  ...     0     0     0     1   \n",
       "4        5   w   s  as  c  d   y  i  m    0  ...     1     0     0     0   \n",
       "...    ...  ..  ..  .. .. ..  .. .. ..  ...  ...   ...   ...   ...   ...   \n",
       "4204  8410  aj   h  as  f  d  aa  j  e    0  ...     0     0     0     0   \n",
       "4205  8411   t  aa  ai  d  d  aa  j  y    0  ...     0     1     0     0   \n",
       "4206  8413   y   v  as  f  d  aa  d  w    0  ...     0     0     0     0   \n",
       "4207  8414  ak   v  as  a  d  aa  c  q    0  ...     0     0     1     0   \n",
       "4208  8416   t  aa  ai  c  d  aa  g  r    0  ...     1     0     0     0   \n",
       "\n",
       "      X379  X380  X382  X383  X384  X385  \n",
       "0        0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...  \n",
       "4204     0     0     0     0     0     0  \n",
       "4205     0     0     0     0     0     0  \n",
       "4206     0     0     0     0     0     0  \n",
       "4207     0     0     0     0     0     0  \n",
       "4208     0     0     0     0     0     0  \n",
       "\n",
       "[4209 rows x 377 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest =  dtest.drop([\"ID\",\"X0\",\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X8\" ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X10  X11  X12  X13  X14  X15  X16  X17  X18  X19  ...  X375  X376  X377  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "1    0    0    0    0    0    0    0    0    0    1  ...     0     0     1   \n",
       "2    0    0    0    0    1    0    0    0    0    0  ...     0     0     0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "4    0    0    0    0    1    0    0    0    0    0  ...     1     0     0   \n",
       "\n",
       "   X378  X379  X380  X382  X383  X384  X385  \n",
       "0     1     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0  \n",
       "2     1     0     0     0     0     0     0  \n",
       "3     1     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 368 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 368)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data.values[0:3499,1:], data.values[0:3499, :1].ravel()\n",
    "X_valid, y_valid = data.values[3500:4208,1:], data.values[3500:4208, :1].ravel()\n",
    "X_test = dtest.values[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train---- (3499, 368)\n",
      "y_train---- (3499,)\n",
      "X_valid---- (708, 368)\n",
      "y_valid---- (708,)\n",
      "X_test----- (4209, 368)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train----\",X_train.shape)\n",
    "print(\"y_train----\",y_train.shape)      \n",
    "print(\"X_valid----\",X_valid.shape)\n",
    "print(\"y_valid----\",y_valid.shape)\n",
    "print(\"X_test-----\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.81"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = min_max_scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred, y_test):\n",
    "    evs = sm.explained_variance_score(y_test, pred)\n",
    "    me = sm.max_error(y_test, pred)\n",
    "    mae = sm.mean_absolute_error(y_test, pred)\n",
    "    mse = sm.mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #msle = sm.mean_squared_log_error(y_test, pred)\n",
    "    m_ae = sm.median_absolute_error(y_test, pred)\n",
    "    r2 = sm.r2_score(y_test, pred)\n",
    "    #mpd = sm.mean_poisson_deviance(y_test, pred)\n",
    "    #mgd = sm.mean_gamma_deviance(y_test, pred)\n",
    "    mape = mean_absolute_percentage_error(pred, y_test)\n",
    "    return({'Explained Variance Score': evs,\n",
    "            'Max Error': me,\n",
    "            'Mean Absolute Error': mae,\n",
    "            'Mean Squared Error': mse,\n",
    "            'Root Mean Squared Error': rmse,\n",
    "            #'Mean Squared Log Error': msle,\n",
    "            'Median Absolute Error': m_ae,\n",
    "            'RÂ² Score': r2,\n",
    "            #'Mean Poisson Deviance': mpd,\n",
    "            #'Mean Gamma Deviance': mgd,\n",
    "            'Mean Absolute Percentage Error': mape\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequences = 16\n",
    "timesteps = X_train.shape[1]//subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], subsequences, 1, timesteps, 1))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], subsequences, 1, timesteps, 1))\n",
    "X_test  = X_test.reshape((X_test.shape[0], subsequences, 1, timesteps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3499, 16, 1, 23, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 708 samples\n",
      "Epoch 1/700\n",
      " - 14s - loss: 100.3774 - val_loss: 98.1014\n",
      "Epoch 2/700\n",
      " - 12s - loss: 98.8591 - val_loss: 93.4738\n",
      "Epoch 3/700\n",
      " - 11s - loss: 80.7156 - val_loss: 12.2144\n",
      "Epoch 4/700\n",
      " - 10s - loss: 37.6997 - val_loss: 44.1166\n",
      "Epoch 5/700\n",
      " - 10s - loss: 35.4292 - val_loss: 15.1573\n",
      "Epoch 6/700\n",
      " - 11s - loss: 18.6540 - val_loss: 18.0773\n",
      "Epoch 7/700\n",
      " - 10s - loss: 14.7664 - val_loss: 11.2950\n",
      "Epoch 8/700\n",
      " - 10s - loss: 11.8276 - val_loss: 9.5821\n",
      "Epoch 9/700\n",
      " - 10s - loss: 10.6802 - val_loss: 10.0042\n",
      "Epoch 10/700\n",
      " - 11s - loss: 10.4086 - val_loss: 10.2098\n",
      "Epoch 11/700\n",
      " - 11s - loss: 10.2599 - val_loss: 9.0468\n",
      "Epoch 12/700\n",
      " - 11s - loss: 9.9957 - val_loss: 9.1253\n",
      "Epoch 13/700\n",
      " - 11s - loss: 9.8780 - val_loss: 9.4162\n",
      "Epoch 14/700\n",
      " - 11s - loss: 9.8987 - val_loss: 9.0202\n",
      "Epoch 15/700\n",
      " - 11s - loss: 9.7635 - val_loss: 8.9910\n",
      "Epoch 16/700\n",
      " - 11s - loss: 9.7652 - val_loss: 9.1599\n",
      "Epoch 17/700\n",
      " - 11s - loss: 9.7927 - val_loss: 9.0811\n",
      "Epoch 18/700\n",
      " - 26s - loss: 9.7672 - val_loss: 8.9428\n",
      "Epoch 19/700\n",
      " - 18s - loss: 9.7163 - val_loss: 9.0122\n",
      "Epoch 20/700\n",
      " - 21s - loss: 9.6894 - val_loss: 9.1569\n",
      "Epoch 21/700\n",
      " - 21s - loss: 9.8866 - val_loss: 8.9467\n",
      "Epoch 22/700\n",
      " - 21s - loss: 9.7526 - val_loss: 8.9070\n",
      "Epoch 23/700\n",
      " - 21s - loss: 9.7551 - val_loss: 9.0714\n",
      "Epoch 24/700\n",
      " - 21s - loss: 9.6462 - val_loss: 8.9271\n",
      "Epoch 25/700\n",
      " - 21s - loss: 9.5858 - val_loss: 8.9575\n",
      "Epoch 26/700\n",
      " - 21s - loss: 9.5892 - val_loss: 8.8074\n",
      "Epoch 27/700\n",
      " - 21s - loss: 9.5556 - val_loss: 8.8281\n",
      "Epoch 28/700\n",
      " - 22s - loss: 9.5448 - val_loss: 8.7772\n",
      "Epoch 29/700\n",
      " - 22s - loss: 9.5236 - val_loss: 8.7864\n",
      "Epoch 30/700\n",
      " - 23s - loss: 9.5275 - val_loss: 8.7843\n",
      "Epoch 31/700\n",
      " - 22s - loss: 9.4939 - val_loss: 8.8987\n",
      "Epoch 32/700\n",
      " - 21s - loss: 9.5303 - val_loss: 9.1332\n",
      "Epoch 33/700\n",
      " - 21s - loss: 9.5593 - val_loss: 8.9626\n",
      "Epoch 34/700\n",
      " - 21s - loss: 9.5205 - val_loss: 8.6851\n",
      "Epoch 35/700\n",
      " - 21s - loss: 9.4240 - val_loss: 8.6617\n",
      "Epoch 36/700\n",
      " - 21s - loss: 9.5772 - val_loss: 8.8480\n",
      "Epoch 37/700\n",
      " - 21s - loss: 9.5154 - val_loss: 9.4123\n",
      "Epoch 38/700\n",
      " - 21s - loss: 9.5058 - val_loss: 8.8143\n",
      "Epoch 39/700\n",
      " - 21s - loss: 9.4500 - val_loss: 8.6145\n",
      "Epoch 40/700\n",
      " - 21s - loss: 9.4333 - val_loss: 8.5735\n",
      "Epoch 41/700\n",
      " - 21s - loss: 9.3055 - val_loss: 8.5815\n",
      "Epoch 42/700\n",
      " - 21s - loss: 9.3050 - val_loss: 8.9243\n",
      "Epoch 43/700\n",
      " - 21s - loss: 9.2928 - val_loss: 8.6547\n",
      "Epoch 44/700\n",
      " - 21s - loss: 9.3166 - val_loss: 8.6446\n",
      "Epoch 45/700\n",
      " - 21s - loss: 9.2357 - val_loss: 8.4891\n",
      "Epoch 46/700\n",
      " - 22s - loss: 9.2009 - val_loss: 8.5263\n",
      "Epoch 47/700\n",
      " - 22s - loss: 9.1799 - val_loss: 8.4634\n",
      "Epoch 48/700\n",
      " - 21s - loss: 9.1806 - val_loss: 8.4134\n",
      "Epoch 49/700\n",
      " - 21s - loss: 9.1098 - val_loss: 8.3959\n",
      "Epoch 50/700\n",
      " - 22s - loss: 9.0697 - val_loss: 8.3183\n",
      "Epoch 51/700\n",
      " - 23s - loss: 9.0560 - val_loss: 8.2824\n",
      "Epoch 52/700\n",
      " - 22s - loss: 8.9859 - val_loss: 8.2286\n",
      "Epoch 53/700\n",
      " - 23s - loss: 8.9551 - val_loss: 8.2170\n",
      "Epoch 54/700\n",
      " - 23s - loss: 9.0178 - val_loss: 8.1534\n",
      "Epoch 55/700\n",
      " - 24s - loss: 8.9044 - val_loss: 8.0325\n",
      "Epoch 56/700\n",
      " - 27s - loss: 8.7441 - val_loss: 7.9130\n",
      "Epoch 57/700\n",
      " - 43s - loss: 8.5960 - val_loss: 7.7021\n",
      "Epoch 58/700\n",
      " - 45s - loss: 8.3508 - val_loss: 7.5708\n",
      "Epoch 59/700\n",
      " - 47s - loss: 7.9935 - val_loss: 6.7548\n",
      "Epoch 60/700\n",
      " - 46s - loss: 7.5338 - val_loss: 6.1342\n",
      "Epoch 61/700\n",
      " - 46s - loss: 6.9403 - val_loss: 5.9907\n",
      "Epoch 62/700\n",
      " - 47s - loss: 6.7744 - val_loss: 5.6543\n",
      "Epoch 63/700\n",
      " - 45s - loss: 6.6133 - val_loss: 6.0567\n",
      "Epoch 64/700\n",
      " - 46s - loss: 6.5538 - val_loss: 5.4042\n",
      "Epoch 65/700\n",
      " - 45s - loss: 6.3859 - val_loss: 5.5733\n",
      "Epoch 66/700\n",
      " - 46s - loss: 6.2300 - val_loss: 5.2500\n",
      "Epoch 67/700\n",
      " - 45s - loss: 6.2460 - val_loss: 6.1873\n",
      "Epoch 68/700\n",
      " - 46s - loss: 6.3329 - val_loss: 6.4564\n",
      "Epoch 69/700\n",
      " - 46s - loss: 6.0449 - val_loss: 5.2303\n",
      "Epoch 70/700\n",
      " - 45s - loss: 5.9393 - val_loss: 5.2017\n",
      "Epoch 71/700\n",
      " - 45s - loss: 6.0904 - val_loss: 5.0741\n",
      "Epoch 72/700\n",
      " - 45s - loss: 5.9783 - val_loss: 5.7301\n",
      "Epoch 73/700\n",
      " - 45s - loss: 6.1479 - val_loss: 5.4607\n",
      "Epoch 74/700\n",
      " - 46s - loss: 5.9871 - val_loss: 5.1108\n",
      "Epoch 75/700\n",
      " - 46s - loss: 5.9195 - val_loss: 4.9878\n",
      "Epoch 76/700\n",
      " - 46s - loss: 6.0712 - val_loss: 4.9518\n",
      "Epoch 77/700\n",
      " - 46s - loss: 5.8548 - val_loss: 5.1212\n",
      "Epoch 78/700\n",
      " - 45s - loss: 5.8302 - val_loss: 5.0932\n",
      "Epoch 79/700\n",
      " - 45s - loss: 5.8542 - val_loss: 5.1081\n",
      "Epoch 80/700\n",
      " - 45s - loss: 5.9242 - val_loss: 4.8720\n",
      "Epoch 81/700\n",
      " - 45s - loss: 5.9896 - val_loss: 4.9390\n",
      "Epoch 82/700\n",
      " - 45s - loss: 5.9207 - val_loss: 5.0804\n",
      "Epoch 83/700\n",
      " - 45s - loss: 6.0644 - val_loss: 5.2316\n",
      "Epoch 84/700\n",
      " - 45s - loss: 5.7047 - val_loss: 5.0226\n",
      "Epoch 85/700\n",
      " - 45s - loss: 5.7727 - val_loss: 6.0474\n",
      "Epoch 86/700\n",
      " - 45s - loss: 5.9365 - val_loss: 6.6878\n",
      "Epoch 87/700\n",
      " - 46s - loss: 6.3273 - val_loss: 6.7411\n",
      "Epoch 88/700\n",
      " - 45s - loss: 6.3433 - val_loss: 4.8856\n",
      "Epoch 89/700\n",
      " - 46s - loss: 6.8714 - val_loss: 5.2050\n",
      "Epoch 90/700\n",
      " - 46s - loss: 6.1491 - val_loss: 5.6111\n",
      "Epoch 91/700\n",
      " - 47s - loss: 5.7905 - val_loss: 5.1993\n",
      "Epoch 92/700\n",
      " - 46s - loss: 5.8220 - val_loss: 4.8096\n",
      "Epoch 93/700\n",
      " - 45s - loss: 5.6702 - val_loss: 4.8732\n",
      "Epoch 94/700\n",
      " - 45s - loss: 5.6279 - val_loss: 4.8780\n",
      "Epoch 95/700\n",
      " - 46s - loss: 5.6718 - val_loss: 5.2845\n",
      "Epoch 96/700\n",
      " - 45s - loss: 5.7472 - val_loss: 5.5190\n",
      "Epoch 97/700\n",
      " - 46s - loss: 5.7262 - val_loss: 5.3021\n",
      "Epoch 98/700\n",
      " - 46s - loss: 5.6083 - val_loss: 5.1357\n",
      "Epoch 99/700\n",
      " - 45s - loss: 5.5979 - val_loss: 4.9169\n",
      "Epoch 100/700\n",
      " - 46s - loss: 5.5691 - val_loss: 5.1358\n",
      "Epoch 101/700\n",
      " - 46s - loss: 5.6930 - val_loss: 5.4897\n",
      "Epoch 102/700\n",
      " - 45s - loss: 5.6740 - val_loss: 4.8606\n",
      "Epoch 103/700\n",
      " - 45s - loss: 5.7419 - val_loss: 4.7564\n",
      "Epoch 104/700\n",
      " - 46s - loss: 5.7646 - val_loss: 4.7612\n",
      "Epoch 105/700\n",
      " - 47s - loss: 5.6371 - val_loss: 4.7581\n",
      "Epoch 106/700\n",
      " - 45s - loss: 5.6342 - val_loss: 4.9108\n",
      "Epoch 107/700\n",
      " - 45s - loss: 5.6164 - val_loss: 5.2944\n",
      "Epoch 108/700\n",
      " - 45s - loss: 5.5556 - val_loss: 5.0621\n",
      "Epoch 109/700\n",
      " - 46s - loss: 5.5638 - val_loss: 4.9422\n",
      "Epoch 110/700\n",
      " - 45s - loss: 5.5653 - val_loss: 4.6993\n",
      "Epoch 111/700\n",
      " - 45s - loss: 5.5867 - val_loss: 4.7136\n",
      "Epoch 112/700\n",
      " - 45s - loss: 5.7499 - val_loss: 4.7051\n",
      "Epoch 113/700\n",
      " - 45s - loss: 5.6583 - val_loss: 4.8445\n",
      "Epoch 114/700\n",
      " - 45s - loss: 5.4925 - val_loss: 5.0081\n",
      "Epoch 115/700\n",
      " - 46s - loss: 5.4869 - val_loss: 5.1736\n",
      "Epoch 116/700\n",
      " - 46s - loss: 5.7248 - val_loss: 4.9120\n",
      "Epoch 117/700\n",
      " - 45s - loss: 5.6291 - val_loss: 4.9130\n",
      "Epoch 118/700\n",
      " - 45s - loss: 5.4810 - val_loss: 5.0627\n",
      "Epoch 119/700\n",
      " - 46s - loss: 5.6161 - val_loss: 4.9905\n",
      "Epoch 120/700\n",
      " - 45s - loss: 5.4960 - val_loss: 4.9145\n",
      "Epoch 121/700\n",
      " - 46s - loss: 5.5450 - val_loss: 5.2078\n",
      "Epoch 122/700\n",
      " - 45s - loss: 5.5954 - val_loss: 5.4582\n",
      "Epoch 123/700\n",
      " - 46s - loss: 5.5860 - val_loss: 5.5302\n",
      "Epoch 124/700\n",
      " - 45s - loss: 5.5154 - val_loss: 5.1140\n",
      "Epoch 125/700\n",
      " - 46s - loss: 5.5647 - val_loss: 5.5111\n",
      "Epoch 126/700\n",
      " - 45s - loss: 5.5691 - val_loss: 5.3424\n",
      "Epoch 127/700\n",
      " - 46s - loss: 5.6312 - val_loss: 5.2796\n",
      "Epoch 128/700\n",
      " - 46s - loss: 5.6926 - val_loss: 4.6324\n",
      "Epoch 129/700\n",
      " - 45s - loss: 5.5519 - val_loss: 4.6239\n",
      "Epoch 130/700\n",
      " - 46s - loss: 5.4158 - val_loss: 4.6791\n",
      "Epoch 131/700\n",
      " - 46s - loss: 5.3978 - val_loss: 4.9127\n",
      "Epoch 132/700\n",
      " - 47s - loss: 5.4514 - val_loss: 5.2975\n",
      "Epoch 133/700\n",
      " - 45s - loss: 5.5610 - val_loss: 5.2047\n",
      "Epoch 134/700\n",
      " - 45s - loss: 5.4083 - val_loss: 5.3724\n",
      "Epoch 135/700\n",
      " - 46s - loss: 5.4384 - val_loss: 5.3122\n",
      "Epoch 136/700\n",
      " - 45s - loss: 5.4224 - val_loss: 5.3904\n",
      "Epoch 137/700\n",
      " - 45s - loss: 5.4341 - val_loss: 5.3801\n",
      "Epoch 138/700\n",
      " - 46s - loss: 5.4974 - val_loss: 5.2480\n",
      "Epoch 139/700\n",
      " - 45s - loss: 5.4367 - val_loss: 5.6072\n",
      "Epoch 140/700\n",
      " - 45s - loss: 5.5412 - val_loss: 5.4512\n",
      "Epoch 141/700\n",
      " - 45s - loss: 5.6529 - val_loss: 5.4044\n",
      "Epoch 142/700\n",
      " - 46s - loss: 5.6193 - val_loss: 4.7379\n",
      "Epoch 143/700\n",
      " - 45s - loss: 5.4841 - val_loss: 4.5025\n",
      "Epoch 144/700\n",
      " - 45s - loss: 5.5393 - val_loss: 4.6288\n",
      "Epoch 145/700\n",
      " - 45s - loss: 5.5198 - val_loss: 4.5610\n",
      "Epoch 146/700\n",
      " - 46s - loss: 5.5365 - val_loss: 4.7725\n",
      "Epoch 147/700\n",
      " - 46s - loss: 5.4123 - val_loss: 5.0890\n",
      "Epoch 148/700\n",
      " - 45s - loss: 5.3062 - val_loss: 4.9936\n",
      "Epoch 149/700\n",
      " - 45s - loss: 5.3199 - val_loss: 4.9915\n",
      "Epoch 150/700\n",
      " - 45s - loss: 5.4285 - val_loss: 4.8991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/700\n",
      " - 45s - loss: 5.6722 - val_loss: 4.4477\n",
      "Epoch 152/700\n",
      " - 46s - loss: 5.4295 - val_loss: 4.4415\n",
      "Epoch 153/700\n",
      " - 45s - loss: 5.4421 - val_loss: 4.5011\n",
      "Epoch 154/700\n",
      " - 45s - loss: 5.3816 - val_loss: 4.6118\n",
      "Epoch 155/700\n",
      " - 45s - loss: 5.2985 - val_loss: 4.4993\n",
      "Epoch 156/700\n",
      " - 45s - loss: 5.2638 - val_loss: 5.3700\n",
      "Epoch 157/700\n",
      " - 46s - loss: 5.4166 - val_loss: 4.6010\n",
      "Epoch 158/700\n",
      " - 46s - loss: 5.2182 - val_loss: 4.7107\n",
      "Epoch 159/700\n",
      " - 45s - loss: 5.3326 - val_loss: 5.2591\n",
      "Epoch 160/700\n",
      " - 45s - loss: 5.4421 - val_loss: 5.3925\n",
      "Epoch 161/700\n",
      " - 45s - loss: 5.4771 - val_loss: 5.3826\n",
      "Epoch 162/700\n",
      " - 45s - loss: 5.4656 - val_loss: 4.7664\n",
      "Epoch 163/700\n",
      " - 45s - loss: 5.3219 - val_loss: 5.7256\n",
      "Epoch 164/700\n",
      " - 45s - loss: 5.6204 - val_loss: 4.5059\n",
      "Epoch 165/700\n",
      " - 46s - loss: 5.4676 - val_loss: 4.4242\n",
      "Epoch 166/700\n",
      " - 45s - loss: 5.3718 - val_loss: 4.6578\n",
      "Epoch 167/700\n",
      " - 45s - loss: 5.3471 - val_loss: 4.9749\n",
      "Epoch 168/700\n",
      " - 45s - loss: 5.2184 - val_loss: 5.0354\n",
      "Epoch 169/700\n",
      " - 45s - loss: 5.2565 - val_loss: 4.3926\n",
      "Epoch 170/700\n",
      " - 46s - loss: 5.1863 - val_loss: 4.7255\n",
      "Epoch 171/700\n",
      " - 45s - loss: 5.2043 - val_loss: 4.4849\n",
      "Epoch 172/700\n",
      " - 45s - loss: 5.3253 - val_loss: 4.5754\n",
      "Epoch 173/700\n",
      " - 45s - loss: 5.2634 - val_loss: 5.5520\n",
      "Epoch 174/700\n",
      " - 45s - loss: 5.4971 - val_loss: 5.6775\n",
      "Epoch 175/700\n",
      " - 45s - loss: 5.4758 - val_loss: 4.3718\n",
      "Epoch 176/700\n",
      " - 46s - loss: 5.4738 - val_loss: 4.4449\n",
      "Epoch 177/700\n",
      " - 45s - loss: 5.2086 - val_loss: 4.7283\n",
      "Epoch 178/700\n",
      " - 45s - loss: 5.2627 - val_loss: 5.5852\n",
      "Epoch 179/700\n",
      " - 45s - loss: 5.3019 - val_loss: 4.3968\n",
      "Epoch 180/700\n",
      " - 46s - loss: 5.3786 - val_loss: 4.5341\n",
      "Epoch 181/700\n",
      " - 46s - loss: 5.6061 - val_loss: 5.1295\n",
      "Epoch 182/700\n",
      " - 45s - loss: 5.4024 - val_loss: 5.5711\n",
      "Epoch 183/700\n",
      " - 45s - loss: 5.3816 - val_loss: 4.8320\n",
      "Epoch 184/700\n",
      " - 46s - loss: 5.2568 - val_loss: 4.3431\n",
      "Epoch 185/700\n",
      " - 45s - loss: 5.2110 - val_loss: 4.6498\n",
      "Epoch 186/700\n",
      " - 45s - loss: 5.2645 - val_loss: 5.2508\n",
      "Epoch 187/700\n",
      " - 45s - loss: 5.2954 - val_loss: 5.3441\n",
      "Epoch 188/700\n",
      " - 45s - loss: 5.2135 - val_loss: 4.3364\n",
      "Epoch 189/700\n",
      " - 45s - loss: 5.2222 - val_loss: 4.3873\n",
      "Epoch 190/700\n",
      " - 45s - loss: 5.1279 - val_loss: 4.4285\n",
      "Epoch 191/700\n",
      " - 45s - loss: 5.1029 - val_loss: 4.4994\n",
      "Epoch 192/700\n",
      " - 46s - loss: 5.1258 - val_loss: 4.2985\n",
      "Epoch 193/700\n",
      " - 46s - loss: 5.1689 - val_loss: 4.3211\n",
      "Epoch 194/700\n",
      " - 45s - loss: 5.2873 - val_loss: 4.5862\n",
      "Epoch 195/700\n",
      " - 45s - loss: 5.1906 - val_loss: 4.8993\n",
      "Epoch 196/700\n",
      " - 45s - loss: 5.1730 - val_loss: 4.6247\n",
      "Epoch 197/700\n",
      " - 45s - loss: 5.0664 - val_loss: 4.6590\n",
      "Epoch 198/700\n",
      " - 45s - loss: 5.0979 - val_loss: 4.2987\n",
      "Epoch 199/700\n",
      " - 45s - loss: 5.2301 - val_loss: 4.4083\n",
      "Epoch 200/700\n",
      " - 45s - loss: 5.1025 - val_loss: 4.3847\n",
      "Epoch 201/700\n",
      " - 45s - loss: 5.1131 - val_loss: 4.8789\n",
      "Epoch 202/700\n",
      " - 46s - loss: 5.1502 - val_loss: 4.6251\n",
      "Epoch 203/700\n",
      " - 45s - loss: 5.0973 - val_loss: 4.4893\n",
      "Epoch 204/700\n",
      " - 45s - loss: 5.1453 - val_loss: 4.3477\n",
      "Epoch 205/700\n",
      " - 45s - loss: 5.1001 - val_loss: 4.2770\n",
      "Epoch 206/700\n",
      " - 46s - loss: 5.0709 - val_loss: 4.3469\n",
      "Epoch 207/700\n",
      " - 45s - loss: 5.0668 - val_loss: 4.8238\n",
      "Epoch 208/700\n",
      " - 45s - loss: 5.1275 - val_loss: 5.4565\n",
      "Epoch 209/700\n",
      " - 46s - loss: 5.2348 - val_loss: 4.4846\n",
      "Epoch 210/700\n",
      " - 45s - loss: 5.0820 - val_loss: 4.2678\n",
      "Epoch 211/700\n",
      " - 45s - loss: 5.1105 - val_loss: 4.6063\n",
      "Epoch 212/700\n",
      " - 45s - loss: 5.0264 - val_loss: 5.1444\n",
      "Epoch 213/700\n",
      " - 46s - loss: 5.1172 - val_loss: 4.3850\n",
      "Epoch 214/700\n",
      " - 46s - loss: 5.0206 - val_loss: 4.3095\n",
      "Epoch 215/700\n",
      " - 45s - loss: 5.1472 - val_loss: 4.8016\n",
      "Epoch 216/700\n",
      " - 45s - loss: 5.2672 - val_loss: 5.1513\n",
      "Epoch 217/700\n",
      " - 45s - loss: 5.1376 - val_loss: 4.5682\n",
      "Epoch 218/700\n",
      " - 46s - loss: 5.0453 - val_loss: 4.5526\n",
      "Epoch 219/700\n",
      " - 45s - loss: 5.0315 - val_loss: 4.5214\n",
      "Epoch 220/700\n",
      " - 45s - loss: 5.0550 - val_loss: 4.2521\n",
      "Epoch 221/700\n",
      " - 45s - loss: 5.1265 - val_loss: 4.2642\n",
      "Epoch 222/700\n",
      " - 46s - loss: 5.1587 - val_loss: 4.8240\n",
      "Epoch 223/700\n",
      " - 46s - loss: 5.1475 - val_loss: 5.0356\n",
      "Epoch 224/700\n",
      " - 46s - loss: 5.1471 - val_loss: 5.0865\n",
      "Epoch 225/700\n",
      " - 46s - loss: 5.1943 - val_loss: 4.2690\n",
      "Epoch 226/700\n",
      " - 45s - loss: 5.1185 - val_loss: 4.2667\n",
      "Epoch 227/700\n",
      " - 45s - loss: 5.2451 - val_loss: 4.3339\n",
      "Epoch 228/700\n",
      " - 45s - loss: 5.1434 - val_loss: 4.9721\n",
      "Epoch 229/700\n",
      " - 45s - loss: 5.0444 - val_loss: 4.5985\n",
      "Epoch 230/700\n",
      " - 45s - loss: 5.0753 - val_loss: 4.4355\n",
      "Epoch 231/700\n",
      " - 45s - loss: 4.9771 - val_loss: 4.3245\n",
      "Epoch 232/700\n",
      " - 45s - loss: 4.9938 - val_loss: 4.3789\n",
      "Epoch 233/700\n",
      " - 46s - loss: 5.0399 - val_loss: 4.4048\n",
      "Epoch 234/700\n",
      " - 45s - loss: 5.2982 - val_loss: 5.6422\n",
      "Epoch 235/700\n",
      " - 45s - loss: 5.3286 - val_loss: 4.4366\n",
      "Epoch 236/700\n",
      " - 45s - loss: 5.4216 - val_loss: 4.3071\n",
      "Epoch 237/700\n",
      " - 45s - loss: 5.5128 - val_loss: 5.6409\n",
      "Epoch 238/700\n",
      " - 45s - loss: 5.2481 - val_loss: 4.5455\n",
      "Epoch 239/700\n",
      " - 46s - loss: 5.0738 - val_loss: 4.5191\n",
      "Epoch 240/700\n",
      " - 46s - loss: 5.0377 - val_loss: 4.2015\n",
      "Epoch 241/700\n",
      " - 46s - loss: 5.0420 - val_loss: 4.6043\n",
      "Epoch 242/700\n",
      " - 45s - loss: 5.0786 - val_loss: 5.1196\n",
      "Epoch 243/700\n",
      " - 45s - loss: 5.0908 - val_loss: 4.3092\n",
      "Epoch 244/700\n",
      " - 45s - loss: 4.9831 - val_loss: 4.1937\n",
      "Epoch 245/700\n",
      " - 45s - loss: 5.0198 - val_loss: 4.5785\n",
      "Epoch 246/700\n",
      " - 45s - loss: 4.9594 - val_loss: 4.7951\n",
      "Epoch 247/700\n",
      " - 45s - loss: 5.0422 - val_loss: 4.7478\n",
      "Epoch 248/700\n",
      " - 45s - loss: 5.0172 - val_loss: 4.1681\n",
      "Epoch 249/700\n",
      " - 45s - loss: 5.0203 - val_loss: 4.3366\n",
      "Epoch 250/700\n",
      " - 45s - loss: 5.0559 - val_loss: 4.8373\n",
      "Epoch 251/700\n",
      " - 46s - loss: 5.0538 - val_loss: 4.2653\n",
      "Epoch 252/700\n",
      " - 45s - loss: 5.1550 - val_loss: 4.2390\n",
      "Epoch 253/700\n",
      " - 45s - loss: 5.4577 - val_loss: 5.8785\n",
      "Epoch 254/700\n",
      " - 46s - loss: 5.4278 - val_loss: 4.3954\n",
      "Epoch 255/700\n",
      " - 46s - loss: 5.1758 - val_loss: 4.1935\n",
      "Epoch 256/700\n",
      " - 45s - loss: 5.0293 - val_loss: 4.7319\n",
      "Epoch 257/700\n",
      " - 45s - loss: 5.0029 - val_loss: 4.4510\n",
      "Epoch 258/700\n",
      " - 45s - loss: 4.9634 - val_loss: 4.2735\n",
      "Epoch 259/700\n",
      " - 45s - loss: 4.9771 - val_loss: 4.4891\n",
      "Epoch 260/700\n",
      " - 45s - loss: 4.9498 - val_loss: 4.5069\n",
      "Epoch 261/700\n",
      " - 45s - loss: 4.9122 - val_loss: 4.1807\n",
      "Epoch 262/700\n",
      " - 46s - loss: 4.9628 - val_loss: 4.6363\n",
      "Epoch 263/700\n",
      " - 45s - loss: 4.8992 - val_loss: 4.3589\n",
      "Epoch 264/700\n",
      " - 45s - loss: 4.8986 - val_loss: 4.1734\n",
      "Epoch 265/700\n",
      " - 45s - loss: 4.8666 - val_loss: 4.4198\n",
      "Epoch 266/700\n",
      " - 45s - loss: 4.9082 - val_loss: 4.5692\n",
      "Epoch 267/700\n",
      " - 46s - loss: 4.9351 - val_loss: 4.5549\n",
      "Epoch 268/700\n",
      " - 45s - loss: 4.9377 - val_loss: 4.1134\n",
      "Epoch 269/700\n",
      " - 45s - loss: 5.0040 - val_loss: 4.3347\n",
      "Epoch 270/700\n",
      " - 45s - loss: 4.9049 - val_loss: 4.5328\n",
      "Epoch 271/700\n",
      " - 45s - loss: 4.9105 - val_loss: 4.6296\n",
      "Epoch 272/700\n",
      " - 45s - loss: 5.0502 - val_loss: 4.2052\n",
      "Epoch 273/700\n",
      " - 45s - loss: 5.2031 - val_loss: 4.6476\n",
      "Epoch 274/700\n",
      " - 45s - loss: 5.0295 - val_loss: 4.5258\n",
      "Epoch 275/700\n",
      " - 46s - loss: 5.0096 - val_loss: 4.2151\n",
      "Epoch 276/700\n",
      " - 46s - loss: 5.1106 - val_loss: 5.0102\n",
      "Epoch 277/700\n",
      " - 45s - loss: 4.9604 - val_loss: 4.4210\n",
      "Epoch 278/700\n",
      " - 45s - loss: 4.8846 - val_loss: 4.1561\n",
      "Epoch 279/700\n",
      " - 46s - loss: 4.9911 - val_loss: 4.1554\n",
      "Epoch 280/700\n",
      " - 46s - loss: 4.8938 - val_loss: 4.2916\n",
      "Epoch 281/700\n",
      " - 46s - loss: 4.8437 - val_loss: 4.4433\n",
      "Epoch 282/700\n",
      " - 45s - loss: 4.8689 - val_loss: 4.2791\n",
      "Epoch 283/700\n",
      " - 46s - loss: 4.8116 - val_loss: 4.2818\n",
      "Epoch 284/700\n",
      " - 45s - loss: 4.8093 - val_loss: 4.1811\n",
      "Epoch 285/700\n",
      " - 45s - loss: 4.8222 - val_loss: 4.1723\n",
      "Epoch 286/700\n",
      " - 46s - loss: 4.8365 - val_loss: 4.6467\n",
      "Epoch 287/700\n",
      " - 45s - loss: 4.8431 - val_loss: 4.1425\n",
      "Epoch 288/700\n",
      " - 46s - loss: 4.8997 - val_loss: 4.3675\n",
      "Epoch 289/700\n",
      " - 45s - loss: 4.8498 - val_loss: 4.6072\n",
      "Epoch 290/700\n",
      " - 45s - loss: 5.0191 - val_loss: 4.9263\n",
      "Epoch 291/700\n",
      " - 45s - loss: 4.9323 - val_loss: 4.0950\n",
      "Epoch 292/700\n",
      " - 45s - loss: 4.8175 - val_loss: 4.3420\n",
      "Epoch 293/700\n",
      " - 46s - loss: 4.8238 - val_loss: 4.1813\n",
      "Epoch 294/700\n",
      " - 45s - loss: 4.8323 - val_loss: 4.4158\n",
      "Epoch 295/700\n",
      " - 45s - loss: 4.7928 - val_loss: 4.2551\n",
      "Epoch 296/700\n",
      " - 45s - loss: 4.8358 - val_loss: 4.5000\n",
      "Epoch 297/700\n",
      " - 45s - loss: 4.8218 - val_loss: 4.7238\n",
      "Epoch 298/700\n",
      " - 46s - loss: 4.8996 - val_loss: 4.5790\n",
      "Epoch 299/700\n",
      " - 46s - loss: 4.8558 - val_loss: 4.1344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/700\n",
      " - 45s - loss: 4.9219 - val_loss: 4.0764\n",
      "Epoch 301/700\n",
      " - 45s - loss: 4.9728 - val_loss: 4.3310\n",
      "Epoch 302/700\n",
      " - 46s - loss: 4.9459 - val_loss: 5.2066\n",
      "Epoch 303/700\n",
      " - 46s - loss: 5.1081 - val_loss: 4.9282\n",
      "Epoch 304/700\n",
      " - 45s - loss: 5.0428 - val_loss: 4.1408\n",
      "Epoch 305/700\n",
      " - 45s - loss: 4.8924 - val_loss: 4.2655\n",
      "Epoch 306/700\n",
      " - 45s - loss: 4.8658 - val_loss: 4.1599\n",
      "Epoch 307/700\n",
      " - 45s - loss: 4.8293 - val_loss: 4.8039\n",
      "Epoch 308/700\n",
      " - 45s - loss: 4.8782 - val_loss: 4.2754\n",
      "Epoch 309/700\n",
      " - 45s - loss: 4.8291 - val_loss: 4.0512\n",
      "Epoch 310/700\n",
      " - 45s - loss: 4.8543 - val_loss: 4.3061\n",
      "Epoch 311/700\n",
      " - 45s - loss: 4.7711 - val_loss: 4.2188\n",
      "Epoch 312/700\n",
      " - 45s - loss: 4.7388 - val_loss: 4.1815\n",
      "Epoch 313/700\n",
      " - 45s - loss: 4.7571 - val_loss: 4.0892\n",
      "Epoch 314/700\n",
      " - 45s - loss: 4.8219 - val_loss: 4.2537\n",
      "Epoch 315/700\n",
      " - 46s - loss: 4.8459 - val_loss: 4.8430\n",
      "Epoch 316/700\n",
      " - 45s - loss: 4.9273 - val_loss: 4.3805\n",
      "Epoch 317/700\n",
      " - 45s - loss: 4.7487 - val_loss: 4.1306\n",
      "Epoch 318/700\n",
      " - 45s - loss: 4.8184 - val_loss: 4.7688\n",
      "Epoch 319/700\n",
      " - 45s - loss: 4.8877 - val_loss: 4.3539\n",
      "Epoch 320/700\n",
      " - 45s - loss: 4.7602 - val_loss: 4.1007\n",
      "Epoch 321/700\n",
      " - 45s - loss: 4.7645 - val_loss: 4.5939\n",
      "Epoch 322/700\n",
      " - 45s - loss: 4.7623 - val_loss: 4.1360\n",
      "Epoch 323/700\n",
      " - 45s - loss: 4.7744 - val_loss: 4.2745\n",
      "Epoch 324/700\n",
      " - 45s - loss: 4.7684 - val_loss: 4.6916\n",
      "Epoch 325/700\n",
      " - 45s - loss: 4.9246 - val_loss: 4.5980\n",
      "Epoch 326/700\n",
      " - 46s - loss: 4.8729 - val_loss: 4.1689\n",
      "Epoch 327/700\n",
      " - 45s - loss: 4.7515 - val_loss: 4.0277\n",
      "Epoch 328/700\n",
      " - 45s - loss: 4.7566 - val_loss: 4.1539\n",
      "Epoch 329/700\n",
      " - 46s - loss: 4.7168 - val_loss: 4.4578\n",
      "Epoch 330/700\n",
      " - 45s - loss: 4.8147 - val_loss: 4.0820\n",
      "Epoch 331/700\n",
      " - 46s - loss: 4.8555 - val_loss: 4.0049\n",
      "Epoch 332/700\n",
      " - 45s - loss: 4.9178 - val_loss: 4.0522\n",
      "Epoch 333/700\n",
      " - 46s - loss: 4.7529 - val_loss: 4.5035\n",
      "Epoch 334/700\n",
      " - 45s - loss: 4.7206 - val_loss: 4.1255\n",
      "Epoch 335/700\n",
      " - 45s - loss: 4.7530 - val_loss: 3.9944\n",
      "Epoch 336/700\n",
      " - 45s - loss: 4.7372 - val_loss: 4.6204\n",
      "Epoch 337/700\n",
      " - 45s - loss: 4.8449 - val_loss: 4.7970\n",
      "Epoch 338/700\n",
      " - 45s - loss: 4.9285 - val_loss: 4.0458\n",
      "Epoch 339/700\n",
      " - 45s - loss: 5.1311 - val_loss: 4.2275\n",
      "Epoch 340/700\n",
      " - 46s - loss: 5.1163 - val_loss: 5.3286\n",
      "Epoch 341/700\n",
      " - 45s - loss: 4.9006 - val_loss: 4.3307\n",
      "Epoch 342/700\n",
      " - 45s - loss: 4.7401 - val_loss: 4.1754\n",
      "Epoch 343/700\n",
      " - 45s - loss: 4.6765 - val_loss: 4.2207\n",
      "Epoch 344/700\n",
      " - 45s - loss: 4.6882 - val_loss: 4.1199\n",
      "Epoch 345/700\n",
      " - 46s - loss: 4.7578 - val_loss: 4.3453\n",
      "Epoch 346/700\n",
      " - 45s - loss: 4.7940 - val_loss: 4.6701\n",
      "Epoch 347/700\n",
      " - 45s - loss: 4.8050 - val_loss: 4.1283\n",
      "Epoch 348/700\n",
      " - 45s - loss: 4.7807 - val_loss: 4.1088\n",
      "Epoch 349/700\n",
      " - 45s - loss: 4.7933 - val_loss: 4.7582\n",
      "Epoch 350/700\n",
      " - 45s - loss: 4.7497 - val_loss: 4.0759\n",
      "Epoch 351/700\n",
      " - 45s - loss: 4.7657 - val_loss: 4.2988\n",
      "Epoch 352/700\n",
      " - 45s - loss: 4.8070 - val_loss: 4.4485\n",
      "Epoch 353/700\n",
      " - 45s - loss: 4.7573 - val_loss: 4.2592\n",
      "Epoch 354/700\n",
      " - 46s - loss: 4.7003 - val_loss: 4.1350\n",
      "Epoch 355/700\n",
      " - 45s - loss: 4.6709 - val_loss: 4.0928\n",
      "Epoch 356/700\n",
      " - 45s - loss: 4.6753 - val_loss: 4.5528\n",
      "Epoch 357/700\n",
      " - 45s - loss: 4.8572 - val_loss: 4.4744\n",
      "Epoch 358/700\n",
      " - 45s - loss: 4.9157 - val_loss: 4.0574\n",
      "Epoch 359/700\n",
      " - 45s - loss: 4.8307 - val_loss: 4.2741\n",
      "Epoch 360/700\n",
      " - 46s - loss: 4.7786 - val_loss: 4.6291\n",
      "Epoch 361/700\n",
      " - 45s - loss: 4.8285 - val_loss: 4.0845\n",
      "Epoch 362/700\n",
      " - 45s - loss: 4.7456 - val_loss: 4.0168\n",
      "Epoch 363/700\n",
      " - 45s - loss: 4.8103 - val_loss: 4.0884\n",
      "Epoch 364/700\n",
      " - 45s - loss: 4.7967 - val_loss: 4.9203\n",
      "Epoch 365/700\n",
      " - 45s - loss: 4.8669 - val_loss: 4.0696\n",
      "Epoch 366/700\n",
      " - 45s - loss: 4.6900 - val_loss: 4.2254\n",
      "Epoch 367/700\n",
      " - 44s - loss: 4.6654 - val_loss: 4.3565\n",
      "Epoch 368/700\n",
      " - 45s - loss: 4.7147 - val_loss: 4.5442\n",
      "Epoch 369/700\n",
      " - 45s - loss: 4.7532 - val_loss: 3.9987\n",
      "Epoch 370/700\n",
      " - 45s - loss: 4.7266 - val_loss: 4.2495\n",
      "Epoch 371/700\n",
      " - 45s - loss: 4.7934 - val_loss: 4.6398\n",
      "Epoch 372/700\n",
      " - 45s - loss: 4.6967 - val_loss: 4.3612\n",
      "Epoch 373/700\n",
      " - 45s - loss: 4.7135 - val_loss: 4.1074\n",
      "Epoch 374/700\n",
      " - 45s - loss: 4.7009 - val_loss: 4.4964\n",
      "Epoch 375/700\n",
      " - 46s - loss: 4.8408 - val_loss: 4.7233\n",
      "Epoch 376/700\n",
      " - 45s - loss: 4.7242 - val_loss: 4.1303\n",
      "Epoch 377/700\n",
      " - 45s - loss: 4.7242 - val_loss: 4.0154\n",
      "Epoch 378/700\n",
      " - 45s - loss: 4.7289 - val_loss: 4.0117\n",
      "Epoch 379/700\n",
      " - 45s - loss: 4.6915 - val_loss: 4.1932\n",
      "Epoch 380/700\n",
      " - 46s - loss: 4.6625 - val_loss: 4.1479\n",
      "Epoch 381/700\n",
      " - 45s - loss: 4.7339 - val_loss: 4.4656\n",
      "Epoch 382/700\n",
      " - 45s - loss: 4.6553 - val_loss: 4.7394\n",
      "Epoch 383/700\n",
      " - 46s - loss: 4.7004 - val_loss: 3.9650\n",
      "Epoch 384/700\n",
      " - 45s - loss: 4.6536 - val_loss: 4.1269\n",
      "Epoch 385/700\n",
      " - 46s - loss: 4.6512 - val_loss: 4.5071\n",
      "Epoch 386/700\n",
      " - 45s - loss: 4.7326 - val_loss: 4.1913\n",
      "Epoch 387/700\n",
      " - 45s - loss: 4.7035 - val_loss: 4.0115\n",
      "Epoch 388/700\n",
      " - 45s - loss: 4.6584 - val_loss: 4.0211\n",
      "Epoch 389/700\n",
      " - 45s - loss: 4.6386 - val_loss: 4.0982\n",
      "Epoch 390/700\n",
      " - 45s - loss: 4.6976 - val_loss: 4.0719\n",
      "Epoch 391/700\n",
      " - 46s - loss: 4.8552 - val_loss: 4.1550\n",
      "Epoch 392/700\n",
      " - 46s - loss: 4.6461 - val_loss: 4.0091\n",
      "Epoch 393/700\n",
      " - 45s - loss: 4.6554 - val_loss: 4.5644\n",
      "Epoch 394/700\n",
      " - 46s - loss: 4.6811 - val_loss: 3.9777\n",
      "Epoch 395/700\n",
      " - 45s - loss: 4.8921 - val_loss: 4.0113\n",
      "Epoch 396/700\n",
      " - 45s - loss: 4.7492 - val_loss: 4.4205\n",
      "Epoch 397/700\n",
      " - 46s - loss: 4.7306 - val_loss: 3.9737\n",
      "Epoch 398/700\n",
      " - 45s - loss: 4.7617 - val_loss: 4.0307\n",
      "Epoch 399/700\n",
      " - 45s - loss: 4.6819 - val_loss: 4.1074\n",
      "Epoch 400/700\n",
      " - 45s - loss: 4.6826 - val_loss: 4.3309\n",
      "Epoch 401/700\n",
      " - 46s - loss: 4.6549 - val_loss: 4.3384\n",
      "Epoch 402/700\n",
      " - 45s - loss: 4.6590 - val_loss: 4.0285\n",
      "Epoch 403/700\n",
      " - 46s - loss: 4.8251 - val_loss: 4.0271\n",
      "Epoch 404/700\n",
      " - 45s - loss: 4.9728 - val_loss: 5.5087\n",
      "Epoch 405/700\n",
      " - 45s - loss: 5.0461 - val_loss: 4.0051\n",
      "Epoch 406/700\n",
      " - 46s - loss: 4.9708 - val_loss: 3.9605\n",
      "Epoch 407/700\n",
      " - 45s - loss: 4.9156 - val_loss: 4.8522\n",
      "Epoch 408/700\n",
      " - 46s - loss: 4.8530 - val_loss: 3.9891\n",
      "Epoch 409/700\n",
      " - 45s - loss: 4.7572 - val_loss: 4.3350\n",
      "Epoch 410/700\n",
      " - 46s - loss: 4.7857 - val_loss: 4.3199\n",
      "Epoch 411/700\n",
      " - 45s - loss: 4.7928 - val_loss: 4.0021\n",
      "Epoch 412/700\n",
      " - 45s - loss: 4.6794 - val_loss: 4.1402\n",
      "Epoch 413/700\n",
      " - 46s - loss: 4.6457 - val_loss: 4.5471\n",
      "Epoch 414/700\n",
      " - 45s - loss: 4.6869 - val_loss: 3.9440\n",
      "Epoch 415/700\n",
      " - 45s - loss: 4.8500 - val_loss: 4.9616\n",
      "Epoch 416/700\n",
      " - 45s - loss: 4.9945 - val_loss: 4.4620\n",
      "Epoch 417/700\n",
      " - 46s - loss: 4.7164 - val_loss: 4.0639\n",
      "Epoch 418/700\n",
      " - 45s - loss: 4.6379 - val_loss: 4.4574\n",
      "Epoch 419/700\n",
      " - 45s - loss: 4.6356 - val_loss: 4.1122\n",
      "Epoch 420/700\n",
      " - 45s - loss: 4.6349 - val_loss: 4.6184\n",
      "Epoch 421/700\n",
      " - 45s - loss: 4.7679 - val_loss: 4.6801\n",
      "Epoch 422/700\n",
      " - 45s - loss: 4.8047 - val_loss: 4.0235\n",
      "Epoch 423/700\n",
      " - 45s - loss: 4.6386 - val_loss: 4.2827\n",
      "Epoch 424/700\n",
      " - 45s - loss: 4.6278 - val_loss: 4.3070\n",
      "Epoch 425/700\n",
      " - 45s - loss: 4.6075 - val_loss: 4.0738\n",
      "Epoch 426/700\n",
      " - 46s - loss: 4.6363 - val_loss: 4.7721\n",
      "Epoch 427/700\n",
      " - 45s - loss: 4.7402 - val_loss: 4.2528\n",
      "Epoch 428/700\n",
      " - 45s - loss: 4.6764 - val_loss: 4.1601\n",
      "Epoch 429/700\n",
      " - 46s - loss: 4.6159 - val_loss: 4.2548\n",
      "Epoch 430/700\n",
      " - 45s - loss: 4.6466 - val_loss: 4.4598\n",
      "Epoch 431/700\n",
      " - 45s - loss: 4.6739 - val_loss: 4.1459\n",
      "Epoch 432/700\n",
      " - 46s - loss: 4.5999 - val_loss: 4.3761\n",
      "Epoch 433/700\n",
      " - 45s - loss: 4.6811 - val_loss: 4.4639\n",
      "Epoch 434/700\n",
      " - 45s - loss: 4.6973 - val_loss: 4.0011\n",
      "Epoch 435/700\n",
      " - 45s - loss: 4.6262 - val_loss: 4.1812\n",
      "Epoch 436/700\n",
      " - 45s - loss: 4.5830 - val_loss: 4.0420\n",
      "Epoch 437/700\n",
      " - 46s - loss: 4.6590 - val_loss: 4.2199\n",
      "Epoch 438/700\n",
      " - 45s - loss: 4.7093 - val_loss: 4.7362\n",
      "Epoch 439/700\n",
      " - 45s - loss: 4.7883 - val_loss: 4.0757\n",
      "Epoch 440/700\n",
      " - 45s - loss: 4.7412 - val_loss: 4.0045\n",
      "Epoch 441/700\n",
      " - 45s - loss: 4.7439 - val_loss: 4.6891\n",
      "Epoch 442/700\n",
      " - 45s - loss: 4.6800 - val_loss: 4.3403\n",
      "Epoch 443/700\n",
      " - 45s - loss: 4.6261 - val_loss: 4.3705\n",
      "Epoch 444/700\n",
      " - 45s - loss: 4.6386 - val_loss: 3.9845\n",
      "Epoch 445/700\n",
      " - 46s - loss: 4.6970 - val_loss: 4.5001\n",
      "Epoch 446/700\n",
      " - 46s - loss: 4.7004 - val_loss: 4.3457\n",
      "Epoch 447/700\n",
      " - 45s - loss: 4.6962 - val_loss: 3.9659\n",
      "Epoch 448/700\n",
      " - 45s - loss: 4.7317 - val_loss: 4.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/700\n",
      " - 45s - loss: 4.8246 - val_loss: 4.1251\n",
      "Epoch 450/700\n",
      " - 46s - loss: 4.6553 - val_loss: 4.0355\n",
      "Epoch 451/700\n",
      " - 45s - loss: 4.6276 - val_loss: 4.4214\n",
      "Epoch 452/700\n",
      " - 46s - loss: 4.6293 - val_loss: 3.9722\n",
      "Epoch 453/700\n",
      " - 45s - loss: 4.6636 - val_loss: 4.4240\n",
      "Epoch 454/700\n",
      " - 45s - loss: 4.6717 - val_loss: 4.4103\n",
      "Epoch 455/700\n",
      " - 45s - loss: 4.7449 - val_loss: 3.9398\n",
      "Epoch 456/700\n",
      " - 45s - loss: 4.7535 - val_loss: 4.0815\n",
      "Epoch 457/700\n",
      " - 45s - loss: 4.6648 - val_loss: 4.5368\n",
      "Epoch 458/700\n",
      " - 45s - loss: 4.6059 - val_loss: 4.1365\n",
      "Epoch 459/700\n",
      " - 46s - loss: 4.6041 - val_loss: 4.1656\n",
      "Epoch 460/700\n",
      " - 46s - loss: 4.5592 - val_loss: 4.0563\n",
      "Epoch 461/700\n",
      " - 47s - loss: 4.5725 - val_loss: 4.1622\n",
      "Epoch 462/700\n",
      " - 47s - loss: 4.5579 - val_loss: 4.0393\n",
      "Epoch 463/700\n",
      " - 46s - loss: 4.5473 - val_loss: 4.3847\n",
      "Epoch 464/700\n",
      " - 47s - loss: 4.5831 - val_loss: 4.1949\n",
      "Epoch 465/700\n",
      " - 47s - loss: 4.5644 - val_loss: 4.2227\n",
      "Epoch 466/700\n",
      " - 47s - loss: 4.5700 - val_loss: 4.0295\n",
      "Epoch 467/700\n",
      " - 47s - loss: 4.5846 - val_loss: 3.9069\n",
      "Epoch 468/700\n",
      " - 46s - loss: 4.6748 - val_loss: 4.2132\n",
      "Epoch 469/700\n",
      " - 46s - loss: 4.5763 - val_loss: 4.4740\n",
      "Epoch 470/700\n",
      " - 46s - loss: 4.6223 - val_loss: 4.3036\n",
      "Epoch 471/700\n",
      " - 44s - loss: 4.6015 - val_loss: 3.9452\n",
      "Epoch 472/700\n",
      " - 37s - loss: 4.6758 - val_loss: 4.0934\n",
      "Epoch 473/700\n",
      " - 34s - loss: 4.5498 - val_loss: 4.3792\n",
      "Epoch 474/700\n",
      " - 34s - loss: 4.5646 - val_loss: 4.1044\n",
      "Epoch 475/700\n",
      " - 34s - loss: 4.5723 - val_loss: 3.9580\n",
      "Epoch 476/700\n",
      " - 34s - loss: 4.6368 - val_loss: 4.1793\n",
      "Epoch 477/700\n",
      " - 33s - loss: 4.6272 - val_loss: 4.4292\n",
      "Epoch 478/700\n",
      " - 34s - loss: 4.5662 - val_loss: 3.9972\n",
      "Epoch 479/700\n",
      " - 34s - loss: 4.7123 - val_loss: 4.5972\n",
      "Epoch 480/700\n",
      " - 34s - loss: 4.8815 - val_loss: 4.8575\n",
      "Epoch 481/700\n",
      " - 34s - loss: 4.7906 - val_loss: 3.9470\n",
      "Epoch 482/700\n",
      " - 33s - loss: 4.6968 - val_loss: 4.2163\n",
      "Epoch 483/700\n",
      " - 34s - loss: 4.6097 - val_loss: 4.0960\n",
      "Epoch 484/700\n",
      " - 34s - loss: 4.6056 - val_loss: 4.3837\n",
      "Epoch 485/700\n",
      " - 34s - loss: 4.6683 - val_loss: 4.4846\n",
      "Epoch 486/700\n",
      " - 34s - loss: 4.6528 - val_loss: 3.9551\n",
      "Epoch 487/700\n",
      " - 33s - loss: 4.6655 - val_loss: 4.5343\n",
      "Epoch 488/700\n",
      " - 34s - loss: 4.6450 - val_loss: 4.3370\n",
      "Epoch 489/700\n",
      " - 34s - loss: 4.6151 - val_loss: 3.9495\n",
      "Epoch 490/700\n",
      " - 34s - loss: 4.5961 - val_loss: 4.6148\n",
      "Epoch 491/700\n",
      " - 34s - loss: 4.6181 - val_loss: 4.2185\n",
      "Epoch 492/700\n",
      " - 34s - loss: 4.5361 - val_loss: 4.2181\n",
      "Epoch 493/700\n",
      " - 34s - loss: 4.5401 - val_loss: 4.1837\n",
      "Epoch 494/700\n",
      " - 34s - loss: 4.5254 - val_loss: 4.0103\n",
      "Epoch 495/700\n",
      " - 34s - loss: 4.5437 - val_loss: 4.2872\n",
      "Epoch 496/700\n",
      " - 35s - loss: 4.6220 - val_loss: 4.0465\n",
      "Epoch 497/700\n",
      " - 33s - loss: 4.5721 - val_loss: 4.0517\n",
      "Epoch 498/700\n",
      " - 34s - loss: 4.5343 - val_loss: 4.0447\n",
      "Epoch 499/700\n",
      " - 34s - loss: 4.5380 - val_loss: 3.9491\n",
      "Epoch 500/700\n",
      " - 34s - loss: 4.5194 - val_loss: 3.9482\n",
      "Epoch 501/700\n",
      " - 34s - loss: 4.6065 - val_loss: 4.0168\n",
      "Epoch 502/700\n",
      " - 33s - loss: 4.5983 - val_loss: 4.1208\n",
      "Epoch 503/700\n",
      " - 34s - loss: 4.5575 - val_loss: 4.1110\n",
      "Epoch 504/700\n",
      " - 34s - loss: 4.5286 - val_loss: 4.4149\n",
      "Epoch 505/700\n",
      " - 34s - loss: 4.5352 - val_loss: 4.1730\n",
      "Epoch 506/700\n",
      " - 33s - loss: 4.5373 - val_loss: 4.2516\n",
      "Epoch 507/700\n",
      " - 34s - loss: 4.5154 - val_loss: 4.2425\n",
      "Epoch 508/700\n",
      " - 34s - loss: 4.5466 - val_loss: 4.4521\n",
      "Epoch 509/700\n",
      " - 34s - loss: 4.6944 - val_loss: 4.1889\n",
      "Epoch 510/700\n",
      " - 34s - loss: 4.6226 - val_loss: 3.9486\n",
      "Epoch 511/700\n",
      " - 34s - loss: 4.7180 - val_loss: 4.8339\n",
      "Epoch 512/700\n",
      " - 34s - loss: 4.5849 - val_loss: 4.4210\n",
      "Epoch 513/700\n",
      " - 34s - loss: 4.5303 - val_loss: 4.2471\n",
      "Epoch 514/700\n",
      " - 34s - loss: 4.5544 - val_loss: 4.2098\n",
      "Epoch 515/700\n",
      " - 35s - loss: 4.5387 - val_loss: 4.0550\n",
      "Epoch 516/700\n",
      " - 34s - loss: 4.5081 - val_loss: 4.0274\n",
      "Epoch 517/700\n",
      " - 34s - loss: 4.5279 - val_loss: 4.0891\n",
      "Epoch 518/700\n",
      " - 34s - loss: 4.5392 - val_loss: 4.2479\n",
      "Epoch 519/700\n",
      " - 34s - loss: 4.6116 - val_loss: 4.9368\n",
      "Epoch 520/700\n",
      " - 34s - loss: 4.7826 - val_loss: 4.0504\n",
      "Epoch 521/700\n",
      " - 34s - loss: 4.5619 - val_loss: 4.0651\n",
      "Epoch 522/700\n",
      " - 34s - loss: 4.5399 - val_loss: 4.2850\n",
      "Epoch 523/700\n",
      " - 34s - loss: 4.5584 - val_loss: 4.3875\n",
      "Epoch 524/700\n",
      " - 34s - loss: 4.5346 - val_loss: 4.2801\n",
      "Epoch 525/700\n",
      " - 34s - loss: 4.4859 - val_loss: 4.2635\n",
      "Epoch 526/700\n",
      " - 34s - loss: 4.5029 - val_loss: 4.0313\n",
      "Epoch 527/700\n",
      " - 34s - loss: 4.5346 - val_loss: 4.5794\n",
      "Epoch 528/700\n",
      " - 34s - loss: 4.5535 - val_loss: 4.1145\n",
      "Epoch 529/700\n",
      " - 34s - loss: 4.5162 - val_loss: 4.0146\n",
      "Epoch 530/700\n",
      " - 34s - loss: 4.5472 - val_loss: 4.3974\n",
      "Epoch 531/700\n",
      " - 34s - loss: 4.5087 - val_loss: 4.0171\n",
      "Epoch 532/700\n",
      " - 34s - loss: 4.5033 - val_loss: 4.0943\n",
      "Epoch 533/700\n",
      " - 34s - loss: 4.5639 - val_loss: 4.0904\n",
      "Epoch 534/700\n",
      " - 34s - loss: 4.4808 - val_loss: 4.3413\n",
      "Epoch 535/700\n",
      " - 34s - loss: 4.5368 - val_loss: 4.3048\n",
      "Epoch 536/700\n",
      " - 34s - loss: 4.5945 - val_loss: 4.0791\n",
      "Epoch 537/700\n",
      " - 34s - loss: 4.5370 - val_loss: 4.1307\n",
      "Epoch 538/700\n",
      " - 34s - loss: 4.5409 - val_loss: 4.2848\n",
      "Epoch 539/700\n",
      " - 34s - loss: 4.5079 - val_loss: 4.0558\n",
      "Epoch 540/700\n",
      " - 34s - loss: 4.5032 - val_loss: 4.0889\n",
      "Epoch 541/700\n",
      " - 34s - loss: 4.5531 - val_loss: 4.8362\n",
      "Epoch 542/700\n",
      " - 34s - loss: 4.5986 - val_loss: 4.1433\n",
      "Epoch 543/700\n",
      " - 34s - loss: 4.6095 - val_loss: 3.9884\n",
      "Epoch 544/700\n",
      " - 34s - loss: 4.5269 - val_loss: 4.0520\n",
      "Epoch 545/700\n",
      " - 34s - loss: 4.4650 - val_loss: 4.4232\n",
      "Epoch 546/700\n",
      " - 34s - loss: 4.5152 - val_loss: 4.0860\n",
      "Epoch 547/700\n",
      " - 35s - loss: 4.5249 - val_loss: 3.9277\n",
      "Epoch 548/700\n",
      " - 34s - loss: 4.5530 - val_loss: 4.1057\n",
      "Epoch 549/700\n",
      " - 34s - loss: 4.4792 - val_loss: 4.6199\n",
      "Epoch 550/700\n",
      " - 34s - loss: 4.5727 - val_loss: 4.0140\n",
      "Epoch 551/700\n",
      " - 34s - loss: 4.5635 - val_loss: 3.9707\n",
      "Epoch 552/700\n",
      " - 34s - loss: 4.5890 - val_loss: 4.0854\n",
      "Epoch 553/700\n",
      " - 34s - loss: 4.4883 - val_loss: 4.2131\n",
      "Epoch 554/700\n",
      " - 34s - loss: 4.4667 - val_loss: 4.3046\n",
      "Epoch 555/700\n",
      " - 34s - loss: 4.4491 - val_loss: 4.2762\n",
      "Epoch 556/700\n",
      " - 33s - loss: 4.4544 - val_loss: 3.9920\n",
      "Epoch 557/700\n",
      " - 34s - loss: 4.5021 - val_loss: 4.0127\n",
      "Epoch 558/700\n",
      " - 34s - loss: 4.5337 - val_loss: 4.2581\n",
      "Epoch 559/700\n",
      " - 34s - loss: 4.6444 - val_loss: 4.9708\n",
      "Epoch 560/700\n",
      " - 34s - loss: 4.6320 - val_loss: 4.1399\n",
      "Epoch 561/700\n",
      " - 33s - loss: 4.5495 - val_loss: 3.9365\n",
      "Epoch 562/700\n",
      " - 34s - loss: 4.5657 - val_loss: 3.9728\n",
      "Epoch 563/700\n",
      " - 34s - loss: 4.5135 - val_loss: 4.2535\n",
      "Epoch 564/700\n",
      " - 34s - loss: 4.5412 - val_loss: 4.6723\n",
      "Epoch 565/700\n",
      " - 34s - loss: 4.4848 - val_loss: 3.9815\n",
      "Epoch 566/700\n",
      " - 34s - loss: 4.5112 - val_loss: 3.9902\n",
      "Epoch 567/700\n",
      " - 34s - loss: 4.4704 - val_loss: 4.3318\n",
      "Epoch 568/700\n",
      " - 34s - loss: 4.4532 - val_loss: 4.4074\n",
      "Epoch 569/700\n",
      " - 34s - loss: 4.4813 - val_loss: 4.0043\n",
      "Epoch 570/700\n",
      " - 34s - loss: 4.5139 - val_loss: 4.0803\n",
      "Epoch 571/700\n",
      " - 34s - loss: 4.4926 - val_loss: 3.9391\n",
      "Epoch 572/700\n",
      " - 34s - loss: 4.5154 - val_loss: 4.2487\n",
      "Epoch 573/700\n",
      " - 34s - loss: 4.4524 - val_loss: 4.3793\n",
      "Epoch 574/700\n",
      " - 34s - loss: 4.4557 - val_loss: 4.3815\n",
      "Epoch 575/700\n",
      " - 34s - loss: 4.4674 - val_loss: 4.2141\n",
      "Epoch 576/700\n",
      " - 34s - loss: 4.4557 - val_loss: 4.1788\n",
      "Epoch 577/700\n",
      " - 34s - loss: 4.4581 - val_loss: 4.4930\n",
      "Epoch 578/700\n",
      " - 34s - loss: 4.4619 - val_loss: 4.0207\n",
      "Epoch 579/700\n",
      " - 35s - loss: 4.4744 - val_loss: 4.6291\n",
      "Epoch 580/700\n",
      " - 34s - loss: 4.5221 - val_loss: 4.5235\n",
      "Epoch 581/700\n",
      " - 34s - loss: 4.5743 - val_loss: 4.1421\n",
      "Epoch 582/700\n",
      " - 34s - loss: 4.7727 - val_loss: 4.0189\n",
      "Epoch 583/700\n",
      " - 34s - loss: 4.7841 - val_loss: 4.9024\n",
      "Epoch 584/700\n",
      " - 34s - loss: 4.6823 - val_loss: 4.2224\n",
      "Epoch 585/700\n",
      " - 34s - loss: 4.5656 - val_loss: 4.0182\n",
      "Epoch 586/700\n",
      " - 34s - loss: 4.5182 - val_loss: 4.3011\n",
      "Epoch 587/700\n",
      " - 34s - loss: 4.4856 - val_loss: 4.2909\n",
      "Epoch 588/700\n",
      " - 34s - loss: 4.5049 - val_loss: 4.1308\n",
      "Epoch 589/700\n",
      " - 33s - loss: 4.5577 - val_loss: 4.3921\n",
      "Epoch 590/700\n",
      " - 34s - loss: 4.5180 - val_loss: 4.1572\n",
      "Epoch 591/700\n",
      " - 34s - loss: 4.5253 - val_loss: 3.9781\n",
      "Epoch 592/700\n",
      " - 34s - loss: 4.5833 - val_loss: 4.3263\n",
      "Epoch 593/700\n",
      " - 34s - loss: 4.4651 - val_loss: 4.5259\n",
      "Epoch 594/700\n",
      " - 34s - loss: 4.4736 - val_loss: 4.4151\n",
      "Epoch 595/700\n",
      " - 34s - loss: 4.4462 - val_loss: 4.0305\n",
      "Epoch 596/700\n",
      " - 34s - loss: 4.4690 - val_loss: 3.9926\n",
      "Epoch 597/700\n",
      " - 34s - loss: 4.4517 - val_loss: 4.1595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598/700\n",
      " - 34s - loss: 4.4695 - val_loss: 4.1713\n",
      "Epoch 599/700\n",
      " - 34s - loss: 4.5044 - val_loss: 4.3897\n",
      "Epoch 600/700\n",
      " - 34s - loss: 4.4607 - val_loss: 4.4797\n",
      "Epoch 601/700\n",
      " - 34s - loss: 4.5170 - val_loss: 4.3627\n",
      "Epoch 602/700\n",
      " - 34s - loss: 4.5220 - val_loss: 3.9837\n",
      "Epoch 603/700\n",
      " - 34s - loss: 4.4608 - val_loss: 4.2783\n",
      "Epoch 604/700\n",
      " - 34s - loss: 4.4581 - val_loss: 4.2441\n",
      "Epoch 605/700\n",
      " - 34s - loss: 4.4638 - val_loss: 4.1427\n",
      "Epoch 606/700\n",
      " - 34s - loss: 4.4967 - val_loss: 4.4404\n",
      "Epoch 607/700\n",
      " - 34s - loss: 4.4506 - val_loss: 4.1767\n",
      "Epoch 608/700\n",
      " - 34s - loss: 4.4710 - val_loss: 4.1419\n",
      "Epoch 609/700\n",
      " - 34s - loss: 4.4783 - val_loss: 4.0741\n",
      "Epoch 610/700\n",
      " - 34s - loss: 4.5045 - val_loss: 4.1849\n",
      "Epoch 611/700\n",
      " - 34s - loss: 4.4339 - val_loss: 4.0438\n",
      "Epoch 612/700\n",
      " - 34s - loss: 4.4685 - val_loss: 3.9465\n",
      "Epoch 613/700\n",
      " - 34s - loss: 4.5849 - val_loss: 4.1559\n",
      "Epoch 614/700\n",
      " - 34s - loss: 4.5505 - val_loss: 5.4626\n",
      "Epoch 615/700\n",
      " - 34s - loss: 4.7786 - val_loss: 4.0283\n",
      "Epoch 616/700\n",
      " - 34s - loss: 4.5563 - val_loss: 3.9680\n",
      "Epoch 617/700\n",
      " - 34s - loss: 4.4530 - val_loss: 4.5123\n",
      "Epoch 618/700\n",
      " - 34s - loss: 4.4255 - val_loss: 4.3695\n",
      "Epoch 619/700\n",
      " - 34s - loss: 4.4608 - val_loss: 4.5458\n",
      "Epoch 620/700\n",
      " - 34s - loss: 4.4295 - val_loss: 4.0321\n",
      "Epoch 621/700\n",
      " - 34s - loss: 4.4802 - val_loss: 4.0306\n",
      "Epoch 622/700\n",
      " - 26s - loss: 4.4819 - val_loss: 4.0926\n",
      "Epoch 623/700\n",
      " - 21s - loss: 4.4901 - val_loss: 3.9716\n",
      "Epoch 624/700\n",
      " - 21s - loss: 4.5708 - val_loss: 4.3851\n",
      "Epoch 625/700\n",
      " - 21s - loss: 4.4616 - val_loss: 4.2000\n",
      "Epoch 626/700\n",
      " - 21s - loss: 4.4169 - val_loss: 4.2223\n",
      "Epoch 627/700\n",
      " - 21s - loss: 4.3860 - val_loss: 4.1086\n",
      "Epoch 628/700\n",
      " - 21s - loss: 4.3832 - val_loss: 4.2508\n",
      "Epoch 629/700\n",
      " - 21s - loss: 4.3892 - val_loss: 4.4899\n",
      "Epoch 630/700\n",
      " - 21s - loss: 4.4166 - val_loss: 4.3481\n",
      "Epoch 631/700\n",
      " - 21s - loss: 4.4131 - val_loss: 4.1448\n",
      "Epoch 632/700\n",
      " - 21s - loss: 4.4086 - val_loss: 4.0214\n",
      "Epoch 633/700\n",
      " - 21s - loss: 4.5513 - val_loss: 4.4413\n",
      "Epoch 634/700\n",
      " - 21s - loss: 4.5544 - val_loss: 5.0141\n",
      "Epoch 635/700\n",
      " - 21s - loss: 4.7327 - val_loss: 4.3101\n",
      "Epoch 636/700\n",
      " - 21s - loss: 4.5129 - val_loss: 4.2051\n",
      "Epoch 637/700\n",
      " - 21s - loss: 4.4202 - val_loss: 4.2441\n",
      "Epoch 638/700\n",
      " - 21s - loss: 4.3659 - val_loss: 4.2741\n",
      "Epoch 639/700\n",
      " - 21s - loss: 4.3706 - val_loss: 4.3722\n",
      "Epoch 640/700\n",
      " - 21s - loss: 4.4257 - val_loss: 3.9764\n",
      "Epoch 641/700\n",
      " - 21s - loss: 4.5139 - val_loss: 4.8274\n",
      "Epoch 642/700\n",
      " - 21s - loss: 4.6017 - val_loss: 4.4661\n",
      "Epoch 643/700\n",
      " - 21s - loss: 4.5340 - val_loss: 4.0387\n",
      "Epoch 644/700\n",
      " - 21s - loss: 4.4373 - val_loss: 4.3015\n",
      "Epoch 645/700\n",
      " - 21s - loss: 4.5024 - val_loss: 4.5656\n",
      "Epoch 646/700\n",
      " - 21s - loss: 4.4948 - val_loss: 4.2206\n",
      "Epoch 647/700\n",
      " - 21s - loss: 4.3841 - val_loss: 4.2793\n",
      "Epoch 648/700\n",
      " - 21s - loss: 4.3788 - val_loss: 4.2796\n",
      "Epoch 649/700\n",
      " - 21s - loss: 4.4575 - val_loss: 4.4093\n",
      "Epoch 650/700\n",
      " - 21s - loss: 4.4458 - val_loss: 4.1810\n",
      "Epoch 651/700\n",
      " - 21s - loss: 4.4343 - val_loss: 4.0241\n",
      "Epoch 652/700\n",
      " - 15s - loss: 4.4216 - val_loss: 4.0704\n",
      "Epoch 653/700\n",
      " - 11s - loss: 4.4021 - val_loss: 4.0864\n",
      "Epoch 654/700\n",
      " - 10s - loss: 4.3886 - val_loss: 4.0941\n",
      "Epoch 655/700\n",
      " - 10s - loss: 4.3999 - val_loss: 4.4513\n",
      "Epoch 656/700\n",
      " - 10s - loss: 4.4061 - val_loss: 4.2748\n",
      "Epoch 657/700\n",
      " - 10s - loss: 4.3861 - val_loss: 4.1111\n",
      "Epoch 658/700\n",
      " - 10s - loss: 4.4072 - val_loss: 4.2969\n",
      "Epoch 659/700\n",
      " - 10s - loss: 4.4117 - val_loss: 4.5372\n",
      "Epoch 660/700\n",
      " - 10s - loss: 4.4534 - val_loss: 4.4790\n",
      "Epoch 661/700\n",
      " - 10s - loss: 4.4275 - val_loss: 4.1890\n",
      "Epoch 662/700\n",
      " - 10s - loss: 4.4102 - val_loss: 4.2047\n",
      "Epoch 663/700\n",
      " - 10s - loss: 4.3882 - val_loss: 4.5768\n",
      "Epoch 664/700\n",
      " - 10s - loss: 4.3861 - val_loss: 4.2867\n",
      "Epoch 665/700\n",
      " - 10s - loss: 4.3874 - val_loss: 4.3955\n",
      "Epoch 666/700\n",
      " - 10s - loss: 4.4061 - val_loss: 4.4397\n",
      "Epoch 667/700\n",
      " - 10s - loss: 4.4120 - val_loss: 4.1674\n",
      "Epoch 668/700\n",
      " - 10s - loss: 4.3824 - val_loss: 3.9525\n",
      "Epoch 669/700\n",
      " - 10s - loss: 4.4755 - val_loss: 4.2149\n",
      "Epoch 670/700\n",
      " - 10s - loss: 4.4220 - val_loss: 4.2380\n",
      "Epoch 671/700\n",
      " - 10s - loss: 4.5059 - val_loss: 4.7256\n",
      "Epoch 672/700\n",
      " - 10s - loss: 4.4593 - val_loss: 4.8664\n",
      "Epoch 673/700\n",
      " - 10s - loss: 4.4945 - val_loss: 4.5605\n",
      "Epoch 674/700\n",
      " - 10s - loss: 4.5278 - val_loss: 4.3292\n",
      "Epoch 675/700\n",
      " - 10s - loss: 4.4166 - val_loss: 4.2795\n",
      "Epoch 676/700\n",
      " - 10s - loss: 4.3922 - val_loss: 4.6978\n",
      "Epoch 677/700\n",
      " - 10s - loss: 4.4373 - val_loss: 4.5046\n",
      "Epoch 678/700\n",
      " - 10s - loss: 4.3974 - val_loss: 4.0558\n",
      "Epoch 679/700\n",
      " - 10s - loss: 4.4472 - val_loss: 4.2271\n",
      "Epoch 680/700\n",
      " - 10s - loss: 4.3836 - val_loss: 4.2218\n",
      "Epoch 681/700\n",
      " - 10s - loss: 4.4207 - val_loss: 4.3184\n",
      "Epoch 682/700\n",
      " - 10s - loss: 4.3595 - val_loss: 4.4493\n",
      "Epoch 683/700\n",
      " - 10s - loss: 4.3936 - val_loss: 4.6350\n",
      "Epoch 684/700\n",
      " - 10s - loss: 4.4056 - val_loss: 4.4525\n",
      "Epoch 685/700\n",
      " - 10s - loss: 4.3994 - val_loss: 4.3270\n",
      "Epoch 686/700\n",
      " - 10s - loss: 4.4183 - val_loss: 4.3777\n",
      "Epoch 687/700\n",
      " - 10s - loss: 4.4119 - val_loss: 4.3163\n",
      "Epoch 688/700\n",
      " - 10s - loss: 4.3585 - val_loss: 4.2901\n",
      "Epoch 689/700\n",
      " - 10s - loss: 4.5063 - val_loss: 3.9877\n",
      "Epoch 690/700\n",
      " - 10s - loss: 4.5165 - val_loss: 4.1531\n",
      "Epoch 691/700\n",
      " - 10s - loss: 4.3551 - val_loss: 4.0966\n",
      "Epoch 692/700\n",
      " - 10s - loss: 4.3978 - val_loss: 4.3514\n",
      "Epoch 693/700\n",
      " - 10s - loss: 4.3707 - val_loss: 4.3539\n",
      "Epoch 694/700\n",
      " - 10s - loss: 4.4613 - val_loss: 4.9965\n",
      "Epoch 695/700\n",
      " - 10s - loss: 4.6151 - val_loss: 3.9037\n",
      "Epoch 696/700\n",
      " - 10s - loss: 4.7610 - val_loss: 4.1679\n",
      "Epoch 697/700\n",
      " - 10s - loss: 4.5760 - val_loss: 4.6188\n",
      "Epoch 698/700\n",
      " - 10s - loss: 4.4306 - val_loss: 4.3854\n",
      "Epoch 699/700\n",
      " - 10s - loss: 4.3590 - val_loss: 4.0757\n",
      "Epoch 700/700\n",
      " - 10s - loss: 4.3573 - val_loss: 4.5678\n"
     ]
    }
   ],
   "source": [
    "modelConvLSTM = Sequential()\n",
    "modelConvLSTM.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', return_sequences=True,input_shape=(X_train.shape[1], 1, X_train.shape[3], X_train.shape[4])))\n",
    "modelConvLSTM.add(ConvLSTM2D(filters=32, kernel_size=(1,2), activation='relu'))\n",
    "modelConvLSTM.add(Flatten())\n",
    "modelConvLSTM.add(Dense(64))\n",
    "modelConvLSTM.add(Dense(32))\n",
    "modelConvLSTM.add(Dense(1))\n",
    "modelConvLSTM.compile(optimizer='adam', loss=\"huber_loss\")\n",
    "history = modelConvLSTM.fit(X_train, y_train, batch_size=512, epochs=700, verbose=2, callbacks=[early_stop], validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Explained Variance Score': 0.667441093049198,\n",
       " 'Max Error': 41.20085632324219,\n",
       " 'Mean Absolute Error': 5.049645117204742,\n",
       " 'Mean Squared Error': 45.42418250680345,\n",
       " 'Root Mean Squared Error': 6.7397464719975515,\n",
       " 'Median Absolute Error': 4.281689758300786,\n",
       " 'RÂ² Score': 0.6551291460523878,\n",
       " 'Mean Absolute Percentage Error': 5.035695447646363}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvLSTMpred = modelConvLSTM.predict(X_valid, verbose=0)\n",
    "ConvLSTMpred = ConvLSTMpred.reshape((ConvLSTMpred.shape[0]))\n",
    "ConvLSTMresults = metrics(ConvLSTMpred, y_valid)\n",
    "ConvLSTMresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvLSTMpred = modelConvLSTM.predict(X_test, verbose=0)\n",
    "ConvLSTMpred = ConvLSTMpred.reshape((ConvLSTMpred.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77.34828 ,  91.77606 ,  75.27989 , ...,  92.2388  , 112.730385,\n",
       "        92.28337 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ConvLSTMpred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
